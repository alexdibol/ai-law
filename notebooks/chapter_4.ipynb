{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMS1n/o6RdTGlfu3F1SY8VR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**AI LAW, CHAPTER 4. INNOVATORS**\n","\n","---"],"metadata":{"id":"WvlVGWFzi3KB"}},{"cell_type":"markdown","source":["##0.REFERENCE"],"metadata":{"id":"Z-2P4HNIjBdw"}},{"cell_type":"markdown","source":["https://claude.ai/share/cf46f461-99e4-4dc5-a885-3591d79c07ba"],"metadata":{"id":"OWUPTQdDkKTh"}},{"cell_type":"markdown","source":["##1.CONTEXT"],"metadata":{"id":"-FcBs1sCjCzD"}},{"cell_type":"markdown","source":["**Introduction to Chapter 4: Building Reusable Legal Assets with Governance**\n","\n","**What You Already Know About AI**\n","\n","If you have used ChatGPT, Claude, or any chatbot application on your phone or computer, you already understand the basic idea of conversational artificial intelligence. You type a question or request, the AI responds with text, and you can continue the conversation back and forth. These tools are remarkably helpful for brainstorming ideas, explaining concepts, drafting emails, or getting quick answers to questions. The experience feels natural and effortless, almost like texting with a knowledgeable friend who is always available.\n","\n","**Why Legal Practice Requires Something Completely Different**\n","\n","However, using AI for actual legal work requires a fundamentally different approach than casual chatbot conversations. When you chat with an AI for personal use, the stakes are low. If it gives you a mediocre recipe suggestion or explains a concept imperfectly, nothing serious happens. You simply ignore the bad advice and try again. But legal practice operates under entirely different constraints that make casual chatbot usage completely inappropriate and potentially dangerous.\n","\n","First, lawyers have ethical obligations to provide competent representation. If you rely on AI-generated work product, you must be able to verify its accuracy, understand its limitations, and take responsibility for its content. A chatbot conversation that disappears after you close the browser cannot meet these requirements. You need documentation showing what you asked, what the AI produced, what risks were identified, and what human review occurred.\n","\n","Second, lawyers handle confidential and privileged information. Typing sensitive client facts into a consumer chatbot may violate your ethical duty to protect confidentiality. Even if the AI provider promises not to store your data, the risk of accidental exposure is unacceptable. You need systems that actively protect private information through techniques like redaction before any data leaves your control.\n","\n","Third, legal work often gets reused and shared. When you create a clause, checklist, policy, or playbook, other lawyers in your firm might use it for different clients and matters. This reuse amplifies any errors or problems in the original work. A mistake in a casual chatbot conversation affects only you. A mistake in a reusable legal asset might affect dozens of clients over many years. This multiplied risk requires proportionally stronger quality assurance.\n","\n","Fourth, lawyers must maintain audit trails for professional responsibility and malpractice defense. If a client sues you, or if a disciplinary authority investigates your conduct, you need contemporaneous records showing what you did and why. A chatbot conversation that leaves no permanent record provides no protection. You need comprehensive logging of inputs, outputs, decisions, and identified risks.\n","\n","Fifth, legal work increasingly faces regulatory scrutiny around AI use. Bar associations, courts, and regulators are developing rules about appropriate AI use in legal practice. Demonstrating compliance requires documentation that consumer chatbots simply do not provide. You need governance artifacts showing that proper safeguards were in place.\n","\n","**What This Notebook Does Differently**\n","\n","This notebook transforms casual AI interaction into professional-grade legal workflow by adding multiple layers of infrastructure that address each concern described above. Instead of typing into a chatbot and hoping for good results, you execute a structured pipeline that generates assets, tests them adversarially, revises them based on test results, and packages them with comprehensive documentation for human review.\n","\n","The notebook creates permanent records of every AI interaction, but stores them in redacted form to protect confidentiality. It automatically flags risks like missing disclaimers or potential hallucinations, aggregating these warnings for systematic review. It generates version-controlled assets so you can track changes from initial draft through revision. It produces human review checklists specifying exactly what a lawyer must verify before using any output. It builds complete audit packages with manifests, logs, statistics, and governance documentation suitable for long-term archival.\n","\n","Most importantly, the notebook never pretends that AI outputs are ready for immediate use. Every asset explicitly states it is a draft requiring human lawyer review. Every asset has verification status set to \"Not verified\" acknowledging that a human attorney must confirm accuracy. Every release package includes a checklist of items requiring human verification. The system treats AI as a drafting assistant that accelerates work, not as an autonomous decision-maker that replaces professional judgment.\n","\n","**Why Chapter 4 Focuses on Reusable Assets**\n","\n","The progression through the book's chapters reflects increasing sophistication in AI use for legal practice. Earlier chapters might have focused on one-time tasks or simple document analysis. Chapter 4 addresses reusable assets, which represent a higher level of practice maturity that this book calls the \"Innovators\" level.\n","\n","Reusable assets are legal work products designed for repeated use across multiple matters: clause libraries that provide tested contract language, playbooks that guide lawyers through complex procedures, checklists that ensure consistent intake or review processes, teaching modules that train students or junior lawyers, and policy templates that can be adapted to different organizational contexts. These assets provide enormous value because the investment in creating and testing them pays dividends across many subsequent uses.\n","\n","However, reusability also creates what the notebook calls increased \"blast radius.\" If a flawed asset gets reused twenty times, that single flaw affects twenty matters. If a poorly tested playbook guides decision-making in fifty cases, all fifty cases inherit any weaknesses in that playbook. This multiplied risk means governance must scale proportionally. You cannot afford to be casual about quality assurance when creating something that will be reused extensively.\n","\n","Chapter 4 implements this scaled governance through systematic adversarial testing. After generating each asset, the notebook creates tests designed to break it, stress it, and find its weaknesses. These include adversarial tests simulating hostile users, edge case tests exploring unusual scenarios, ambiguity tests checking behavior with unclear facts, prompt injection tests attempting to manipulate the asset, and consistency tests verifying internal coherence. Running these tests before any human even sees the asset provides an early quality filter that catches many problems automatically.\n","\n","The chapter also implements proper release management. Assets move from version zero point one draft, through testing and revision to version zero point two, and finally to a release package with explicit readiness assessment. Each version is preserved, creating clear lineage. The release manifest specifies what human reviews are required, what deployment constraints apply, and what verification questions must be answered. This structured progression prevents premature deployment of untested work.\n","\n","**What You Will Learn**\n","\n","By working through this notebook, you will understand how to transform AI from a casual chatbot into a governed professional tool. You will see how redaction protects confidential information before it reaches the AI. You will experience how structured prompting and technical techniques like prefill enforcement ensure reliable outputs. You will observe systematic testing revealing asset weaknesses that human review might miss. You will examine comprehensive governance artifacts that create audit trails for professional accountability.\n","\n","More fundamentally, you will internalize a professional mindset about AI use in legal practice. AI is powerful but requires careful handling. Outputs are helpful but need verification. Automation increases efficiency but does not eliminate professional responsibility. Technology amplifies capability but also amplifies risk. Proper use requires infrastructure, discipline, and sustained attention to governance.\n","\n","**Who Should Use This Notebook**\n","\n","This notebook is designed for United States-based practicing lawyers with minimal AI background. You do not need to understand the technical details of how large language models work. You do not need programming expertise beyond running cells in Google Colab. You do not need previous experience with the Anthropic API or Claude models.\n","\n","What you do need is appreciation for professional responsibility in legal practice, understanding that AI outputs require human verification, willingness to examine governance artifacts and audit trails, and commitment to following proper procedures rather than cutting corners for convenience.\n","\n","**The Path Forward**\n","\n","As you proceed through the ten sections of this notebook, you will build increasingly sophisticated infrastructure culminating in a complete asset development and release pipeline. Each section adds capability while maintaining all previous safeguards. By the end, you will have executed the pipeline on four demonstration cases, created your own custom asset interactively, and generated a comprehensive audit bundle documenting everything that occurred.\n","\n","This is not the easiest way to use AI for legal work. It would be much simpler to just type questions into ChatGPT and copy the responses. But simple is not the same as appropriate. Legal practice demands more than convenience. It demands competence, confidentiality, quality assurance, accountability, and documented compliance with professional standards. This notebook shows you how to meet those demands while still benefiting from AI's remarkable capabilities. The complexity you will encounter reflects the seriousness with which the legal profession must approach these powerful new tools."],"metadata":{"id":"_Ml4fNUCkLN5"}},{"cell_type":"markdown","source":["##2.LIBRARIES AND ENVIRONMENT"],"metadata":{"id":"eG_Lx09fjEuz"}},{"cell_type":"code","source":["# Install dependencies and create run directory\n","\n","!pip install -q anthropic\n","\n","import os\n","import json\n","import re\n","import hashlib\n","from datetime import datetime\n","from pathlib import Path\n","import shutil\n","\n","# Create timestamped run directory\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","RUN_DIR = Path(f\"/content/ai_law_ch4_runs/run_{timestamp}\")\n","RUN_DIR.mkdir(parents=True, exist_ok=True)\n","\n","DELIVERABLES_DIR = RUN_DIR / \"deliverables\"\n","DELIVERABLES_DIR.mkdir(exist_ok=True)\n","\n","print(f\"‚úÖ Run directory created: {RUN_DIR}\")\n","print(f\"‚úÖ Deliverables directory: {DELIVERABLES_DIR}\")\n","print(f\"‚úÖ Timestamp: {timestamp}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAYzWd98uvve","executionInfo":{"status":"ok","timestamp":1767877843325,"user_tz":360,"elapsed":3960,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"689f7080-9790-47d9-e3df-d3354e715bee"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Run directory created: /content/ai_law_ch4_runs/run_20260108_131043\n","‚úÖ Deliverables directory: /content/ai_law_ch4_runs/run_20260108_131043/deliverables\n","‚úÖ Timestamp: 20260108_131043\n"]}]},{"cell_type":"markdown","source":["##3.API SETUP AND CLIENT INITIALIZATION"],"metadata":{"id":"QdO9R5i3jKgk"}},{"cell_type":"markdown","source":["###3.1.OVERVIEW"],"metadata":{"id":"A6Rac3CPjMYb"}},{"cell_type":"markdown","source":["**API Key Setup and Client Initialization**\n","\n","This section establishes the connection between your Google Colab notebook and the Anthropic API service. Think of it as setting up a phone line before making a call - you need the right credentials and connection details to communicate with Claude.\n","\n","**What Happens in This Section**\n","\n","First, the notebook retrieves your Anthropic API key from Google Colab's secure storage system called \"Secrets\". This is similar to retrieving a password from a password manager rather than writing it directly in your code. The key acts as your authorization credential, proving you have permission to use the Claude API service.\n","\n","Next, the system stores this key in an environment variable. Environment variables are temporary storage locations that programs can access during their execution. This makes the key available to other parts of the notebook without repeatedly typing it.\n","\n","Then, the code creates a \"client\" object using the Anthropic library. The client is your communication interface - it handles all the technical details of sending requests to Claude and receiving responses. Without this client, your notebook cannot interact with the AI model.\n","\n","Finally, the section specifies which Claude model to use. In this notebook, we use Claude Haiku version four point five. Different models have different capabilities, speeds, and costs. Haiku is designed for efficiency while maintaining high quality output, making it suitable for production legal workflows where you need reliable performance.\n","\n","**Why This Matters for Legal Practice**\n","\n","For lawyers using AI tools, proper API initialization is a governance requirement, not just a technical step. The approach here demonstrates several best practices. First, keeping API keys in secure storage rather than hardcoding them prevents accidental exposure if you share the notebook. Second, explicitly declaring which model version you use creates an audit trail - six months later, you can verify exactly which AI system generated a particular output. Third, the error handling ensures you receive clear feedback if something goes wrong during setup, rather than mysterious failures later in the workflow.\n","\n","**What You See When Running**\n","\n","When this section executes successfully, you will see three confirmation messages indicating the API key loaded correctly, displaying the model name, and confirming the client initialized. If there is a problem, you will see an error message directing you to add your API key to Colab Secrets using the key icon in the sidebar. This immediate feedback helps you catch configuration issues before attempting to generate any legal assets.\n","\n","**Connection to the Overall Workflow**\n","\n","Everything that follows in this notebook depends on this initialization. Without a properly configured client and model specification, the asset generation pipeline cannot function. This section is the foundation that enables all subsequent governance-tracked AI interactions."],"metadata":{"id":"X3Udow06oXFG"}},{"cell_type":"markdown","source":["###3.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"-KwUcrNBjOP3"}},{"cell_type":"code","source":["# API key setup and client initialization\n","\n","import anthropic\n","from google.colab import userdata\n","\n","try:\n","    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n","    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n","\n","    client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n","    MODEL = \"claude-haiku-4-5-20251001\"\n","\n","    print(\"‚úÖ API key loaded successfully\")\n","    print(f\"‚úÖ Model: {MODEL}\")\n","    print(f\"‚úÖ Client initialized\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Error loading API key: {e}\")\n","    print(\"Please add ANTHROPIC_API_KEY to Colab Secrets (üîë icon in sidebar)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"brutIACXnhNK","executionInfo":{"status":"ok","timestamp":1767875963058,"user_tz":360,"elapsed":3466,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"5d609b31-918a-43a5-aacc-a5e349108ea0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ API key loaded successfully\n","‚úÖ Model: claude-haiku-4-5-20251001\n","‚úÖ Client initialized\n"]}]},{"cell_type":"markdown","source":["##4.GOVERNANCE UTILITIES"],"metadata":{"id":"qyA9WzYLjQvV"}},{"cell_type":"markdown","source":["###4.1.OVERVIEW"],"metadata":{"id":"DUJeUPvLjTsI"}},{"cell_type":"markdown","source":["**Governance Utilities: Manifest, Logging, and Risk Tracking**\n","\n","This section creates the infrastructure for tracking, documenting, and auditing every action the notebook performs. Think of it as setting up a detailed filing system before beginning a complex legal matter - you establish the record-keeping framework first, then populate it as work progresses.\n","\n","**Creating the Run Manifest**\n","\n","The notebook begins by creating a run manifest, which is a master record document for this specific execution session. The manifest captures essential metadata including a unique run identifier based on the timestamp, the chapter and purpose of the notebook, which AI model is being used, and when the session started. This manifest serves as the table of contents for your entire audit package. Later, when reviewing outputs or responding to questions about how an asset was created, you can refer back to this manifest to understand the context.\n","\n","**Initializing the Prompts Log**\n","\n","Next, the system creates a prompts log file using a format called JSONL, which stands for JSON Lines. This log will record every interaction with the Claude API throughout the notebook execution. Critically, all logged content is redacted before storage, meaning personally identifiable information is removed. Each log entry includes a timestamp, a cryptographic hash of the prompt for verification purposes, the redacted prompt text, the redacted response, and how many risks were flagged. This creates a complete but privacy-protected audit trail of all AI interactions.\n","\n","**Setting Up Risk Tracking**\n","\n","The notebook then initializes a risk log as a centralized repository for all identified risks across every API call. As the notebook executes, any time the system detects potential issues - such as missing disclaimers, possible hallucinations, or overconfident language - these observations are logged here with their severity level and explanatory notes. This aggregation allows you to review all risks in one location rather than searching through individual case files.\n","\n","**Capturing the Environment**\n","\n","The system also saves a complete list of installed Python packages and their versions using pip freeze. This technical snapshot ensures reproducibility. If you need to recreate these exact results months later, or if you need to troubleshoot unexpected behavior, you can verify the software environment matches what was used originally.\n","\n","**Global Statistics Tracker**\n","\n","Finally, the section initializes a statistics tracker that counts key metrics throughout execution: total API calls made, total risks logged, cases that succeeded or failed, and pipeline stages that succeeded or failed. These statistics provide quantitative insight into the notebook's performance and help identify patterns or problems.\n","\n","**Why This Infrastructure Matters**\n","\n","For legal practice, this governance infrastructure addresses a fundamental challenge when using AI tools: demonstrating due diligence and maintaining accountability. If opposing counsel questions how an asset was created, or if a supervising attorney needs to verify proper procedures were followed, these logs provide documentary evidence. The manifest shows what was attempted, the prompts log shows what was sent and received, the risk log shows what concerns were identified, and the statistics show overall performance.\n","\n","**Visible Output**\n","\n","When this section runs, you see confirmation messages displaying the file paths where each governance artifact was created. These paths show you exactly where to find the manifest, prompts log, risk log, and environment snapshot. This transparency ensures you know where your audit materials are stored from the very beginning of the process."],"metadata":{"id":"8SSmrUtfkRo3"}},{"cell_type":"markdown","source":["###4.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"k2xcl99ijWWi"}},{"cell_type":"code","source":["# Governance utilities: manifest, logging, risk tracking\n","\n","# Initialize run manifest\n","run_manifest = {\n","    \"run_id\": timestamp,\n","    \"chapter\": \"4_innovators\",\n","    \"model\": MODEL,\n","    \"start_time\": datetime.now().isoformat(),\n","    \"run_directory\": str(RUN_DIR),\n","    \"purpose\": \"Reusable legal asset creation with adversarial testing and release pipeline\"\n","}\n","\n","manifest_path = RUN_DIR / \"run_manifest.json\"\n","with open(manifest_path, 'w') as f:\n","    json.dump(run_manifest, f, indent=2)\n","\n","# Initialize prompts log (JSONL format)\n","prompts_log_path = RUN_DIR / \"prompts_log.jsonl\"\n","prompts_log_path.touch()\n","\n","# Initialize risk log\n","risk_log = {\n","    \"run_id\": timestamp,\n","    \"risks\": []\n","}\n","risk_log_path = RUN_DIR / \"risk_log.json\"\n","with open(risk_log_path, 'w') as f:\n","    json.dump(risk_log, f, indent=2)\n","\n","# Save pip freeze for reproducibility\n","pip_freeze_path = RUN_DIR / \"pip_freeze.txt\"\n","!pip freeze > {pip_freeze_path}\n","\n","print(f\"‚úÖ Run manifest: {manifest_path}\")\n","print(f\"‚úÖ Prompts log: {prompts_log_path}\")\n","print(f\"‚úÖ Risk log: {risk_log_path}\")\n","print(f\"‚úÖ Pip freeze: {pip_freeze_path}\")\n","\n","# Global stats tracker\n","stats = {\n","    \"total_api_calls\": 0,\n","    \"total_risks_logged\": 0,\n","    \"cases_success\": 0,\n","    \"cases_fail\": 0,\n","    \"stages_success\": 0,\n","    \"stages_fail\": 0\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fHdQUyKcm3ff","executionInfo":{"status":"ok","timestamp":1767875968774,"user_tz":360,"elapsed":1309,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"06cfdb1c-2276-48ef-84b6-d4f67f853a24"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Run manifest: /content/ai_law_ch4_runs/run_20260108_123554/run_manifest.json\n","‚úÖ Prompts log: /content/ai_law_ch4_runs/run_20260108_123554/prompts_log.jsonl\n","‚úÖ Risk log: /content/ai_law_ch4_runs/run_20260108_123554/risk_log.json\n","‚úÖ Pip freeze: /content/ai_law_ch4_runs/run_20260108_123554/pip_freeze.txt\n"]}]},{"cell_type":"markdown","source":["##5.REDACTION AND MINIMUM NECESSARY INTAKE UTILITIES\n"],"metadata":{"id":"Rdk2NsbajYha"}},{"cell_type":"markdown","source":["###5.1.OVERVIEW"],"metadata":{"id":"Ri4JhmeGjZ3i"}},{"cell_type":"markdown","source":["**Redaction and Minimum-Necessary Intake Utilities**\n","\n","This section implements privacy protection mechanisms that prevent sensitive client information from being inadvertently exposed during AI interactions. Think of it as establishing attorney-client privilege safeguards before handling confidential materials - you create protective barriers first, then work within those boundaries.\n","\n","**Understanding the Redaction Function**\n","\n","The redaction function scans text for common patterns of personally identifiable information and replaces them with placeholder labels. Specifically, it searches for email addresses, telephone numbers in United States formats, Social Security numbers, and street addresses. When it finds these patterns, it substitutes them with labels like EMAIL REDACTED or PHONE REDACTED. The function also tracks what types of information it removed, returning both the cleaned text and a list of redacted categories.\n","\n","**Critical Limitations**\n","\n","The notebook explicitly warns that redaction is imperfect and operates on a best-effort basis. Pattern-based redaction cannot catch everything. Names without accompanying identifiers, case-specific facts that could identify individuals, or sensitive information in unusual formats may pass through undetected. This is why the notebook includes prominent warnings: do not paste actual sensitive client data into the system. The redaction layer provides a safety net, not a guarantee.\n","\n","**Why Pattern-Based Redaction**\n","\n","The approach uses regular expressions, which are text-matching patterns, to identify common information formats. For example, an email pattern looks for text structured like words at symbol words dot words. A phone number pattern looks for three digits, optional separator, three digits, optional separator, four digits. This method catches standard formats reliably but cannot understand context or identify information presented in non-standard ways.\n","\n","**Minimum-Necessary Fields Function**\n","\n","The section also includes a utility for filtering data dictionaries to keep only required fields. This implements the principle of data minimization - only sending the minimum information necessary to accomplish the task. If you have a data structure with twenty fields but only need five for a particular API call, this function strips away the unnecessary fifteen, reducing exposure risk.\n","\n","**The Demonstration**\n","\n","The section runs a live demonstration using fake data that includes an email address, phone number, street address, and Social Security number. You see the original text, then the redacted version with placeholders, and finally a summary of what was removed. This concrete example helps you understand exactly what the redaction function does and does not catch.\n","\n","**Implications for Legal Practice**\n","\n","For lawyers, this section addresses a fundamental tension when using AI tools: you need enough factual context for useful outputs, but you must protect confidential and privileged information. The redaction approach here represents one strategy - automatic scrubbing of obvious identifiers before any data leaves your control. However, the notebook's warnings emphasize that technology alone cannot ensure confidentiality. Sound judgment about what information to include remains essential.\n","\n","**Warning Messages**\n","\n","The section concludes with a clear warning that appears every time it runs, reminding you that redaction is imperfect. This repeated warning serves an important function: it keeps the limitation front of mind rather than letting you become complacent. The system does not hid"],"metadata":{"id":"oMEx3BRrkT9r"}},{"cell_type":"markdown","source":["###5.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"HJDPCKaZjcEB"}},{"cell_type":"code","source":["# Redaction and minimum-necessary intake utilities\n","\n","def redact(text):\n","    \"\"\"Best-effort redaction of PII. NOT PERFECT - do not paste sensitive data.\"\"\"\n","    if not text:\n","        return text\n","\n","    redacted = text\n","    removed = []\n","\n","    # Email addresses\n","    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n","    if re.search(email_pattern, redacted):\n","        redacted = re.sub(email_pattern, '[EMAIL_REDACTED]', redacted)\n","        removed.append('emails')\n","\n","    # Phone numbers (US format)\n","    phone_pattern = r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b'\n","    if re.search(phone_pattern, redacted):\n","        redacted = re.sub(phone_pattern, '[PHONE_REDACTED]', redacted)\n","        removed.append('phone_numbers')\n","\n","    # SSN format\n","    ssn_pattern = r'\\b\\d{3}-\\d{2}-\\d{4}\\b'\n","    if re.search(ssn_pattern, redacted):\n","        redacted = re.sub(ssn_pattern, '[SSN_REDACTED]', redacted)\n","        removed.append('ssn')\n","\n","    # Street addresses (best effort)\n","    address_pattern = r'\\b\\d{1,5}\\s+[A-Z][a-z]+\\s+(Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Lane|Ln|Drive|Dr)\\b'\n","    if re.search(address_pattern, redacted, re.IGNORECASE):\n","        redacted = re.sub(address_pattern, '[ADDRESS_REDACTED]', redacted, flags=re.IGNORECASE)\n","        removed.append('addresses')\n","\n","    return redacted, removed\n","\n","def minimum_necessary_fields(data_dict, required_fields):\n","    \"\"\"Keep only required fields from input dict.\"\"\"\n","    return {k: v for k, v in data_dict.items() if k in required_fields}\n","\n","# Demo with fake data\n","demo_text = \"\"\"\n","Client John Doe contacted us at john.doe@example.com or 555-123-4567.\n","His address is 123 Main Street, and SSN is 123-45-6789.\n","Meeting scheduled for next Tuesday.\n","\"\"\"\n","\n","redacted_demo, removed_items = redact(demo_text)\n","\n","print(\"BEFORE REDACTION:\")\n","print(demo_text)\n","print(\"\\n\" + \"=\"*60 + \"\\n\")\n","print(\"AFTER REDACTION:\")\n","print(redacted_demo)\n","print(\"\\n\" + \"=\"*60 + \"\\n\")\n","print(f\"Removed fields: {removed_items}\")\n","print(\"\\n‚ö†Ô∏è  WARNING: Redaction is imperfect. Do NOT paste sensitive client data.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdY_DwOFjdx1","executionInfo":{"status":"ok","timestamp":1767875996973,"user_tz":360,"elapsed":21,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"5fddff5a-fcd6-4238-a89d-675c8df164ac"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["BEFORE REDACTION:\n","\n","Client John Doe contacted us at john.doe@example.com or 555-123-4567.\n","His address is 123 Main Street, and SSN is 123-45-6789.\n","Meeting scheduled for next Tuesday.\n","\n","\n","============================================================\n","\n","AFTER REDACTION:\n","\n","Client John Doe contacted us at [EMAIL_REDACTED] or [PHONE_REDACTED].\n","His address is [ADDRESS_REDACTED], and SSN is [SSN_REDACTED].\n","Meeting scheduled for next Tuesday.\n","\n","\n","============================================================\n","\n","Removed fields: ['emails', 'phone_numbers', 'ssn', 'addresses']\n","\n","‚ö†Ô∏è  WARNING: Redaction is imperfect. Do NOT paste sensitive client data.\n"]}]},{"cell_type":"markdown","source":["##6.CLAUDE WRAPPER"],"metadata":{"id":"37Md4ig9jePY"}},{"cell_type":"markdown","source":["###6.1.OVERVIEW"],"metadata":{"id":"CFUHJxTnjf9a"}},{"cell_type":"markdown","source":["**Critical: Claude JSON API with Prefill Enforcement**\n","\n","This section implements the most technically sophisticated and important component of the entire notebook: a reliable method for obtaining structured JSON outputs from Claude. Previous chapters encountered serious failures where the AI model wrapped JSON in explanatory text or markdown formatting, breaking the parsing process. This section solves that problem using a technique called prefill enforcement combined with multiple fallback strategies.\n","\n","**The Core Problem**\n","\n","Large language models are conversational by nature. When you ask them for JSON, they often want to be helpful by explaining what they are doing, adding context, or formatting the output nicely with code fences. For a human reader, this is friendly. For a computer parser expecting pure JSON, this breaks everything. The challenge is forcing the model into a pure structured output mode without triggering its conversational instincts.\n","\n","**The Prefill Solution**\n","\n","Prefill is an advanced API technique where you provide the beginning of the assistant's response as part of your request. In this implementation, the system sends the opening brace character as a prefilled assistant message. The model then continues from that point, completing the JSON object. Because it starts mid-JSON, it cannot add preamble text before the opening brace. This dramatically increases reliability because the model's first token is already committed to being part of the JSON structure.\n","\n","**Why This Works**\n","\n","The prefill technique leverages how the API processes messages. When you include an assistant message with partial content, the model treats that as already-generated output and continues from there. By starting with an opening brace, you force the model into completion mode rather than explanation mode. The model's training makes it highly likely to complete valid JSON once it has started the structure.\n","\n","**Fallback Extraction Strategies**\n","\n","Despite prefill's effectiveness, the notebook implements four additional extraction strategies as safety nets. Strategy one attempts to parse the text directly as received. Strategy two finds the first opening brace and last closing brace, extracting everything between them. Strategy three strips markdown code fence markers that sometimes appear. Strategy four performs bracket-balancing scanning to find the first complete JSON object in the text. These layers ensure that even if prefill occasionally fails, the system can still recover valid JSON.\n","\n","**Schema Validation**\n","\n","After successfully parsing JSON, the system validates that all required top-level keys are present. The schema specifies exactly what structure the JSON must have: task description, facts provided, assumptions made, open questions, asset details, tests, release readiness assessment, risks, verification status, and questions requiring verification. If any required key is missing, the validation fails and the system attempts a retry.\n","\n","**Automatic Risk Flagging**\n","\n","The system automatically scans the returned JSON for concerning patterns. If the response contains words like cite, statute, case, or regulation, it flags a potential hallucination risk because the model should never invent legal authorities. If the verification status is not set to \"Not verified\", it flags overconfidence. If the asset content lacks required disclaimers about being a draft and not legal advice, it flags missing safeguards. These automatic checks catch common failure modes without requiring manual review of every output.\n","\n","**Retry Logic with Progressive Strictness**\n","\n","If the first attempt fails, the system makes up to three total attempts with increasing strictness. The first attempt uses temperature zero point one, allowing slight creativity. Subsequent attempts use temperature zero, making the model more deterministic, and add an explicit prefix demanding JSON only with no text. This progressive approach gives the model multiple chances while tightening constraints each time.\n","\n","**Error Fallback Schema**\n","\n","If all three attempts fail, rather than crashing the notebook, the system returns a valid error schema that matches the expected structure. This error schema has task set to \"error fallback\", includes the failure message in the risks array with high severity, and sets release readiness to needs revision. This approach ensures the pipeline can continue even after failures, documenting what went wrong rather than halting execution.\n","\n","**Logging and Risk Tracking**\n","\n","Every successful API call triggers logging to the prompts log file and updates to the risk log. The system redacts both the prompt and response before logging, applies cryptographic hashing to the prompt for verification purposes, and records how many risks were flagged. This creates the audit trail that governance requires while protecting confidential information.\n","\n","**The Smoke Test**\n","\n","The section concludes with a smoke test that validates the entire prefill mechanism before the main pipeline runs. It sends a simple prompt requesting a client intake checklist, then verifies the response has the asset key, verification status equals \"Not verified\", and content includes the draft disclaimer. This immediate validation catches configuration problems early rather than discovering them deep into pipeline execution.\n","\n","**Why This Complexity Matters**\n","\n","For legal practitioners, this technical infrastructure might seem excessive. However, reliability is not optional when generating reusable legal assets. If the system produces malformed outputs that require manual intervention, it defeats the purpose of automation. If it occasionally invents citations or omits disclaimers, it creates liability exposure. The elaborate defensive programming here ensures consistent, governable, auditable outputs suitable for professional legal practice."],"metadata":{"id":"nI6BO3C4kXRG"}},{"cell_type":"markdown","source":["###6.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"kCgz7QJjjhwA"}},{"cell_type":"code","source":["# CRITICAL: Claude JSON API with PREFILL enforcement\n","\n","SYSTEM_PROMPT = \"\"\"Output ONLY JSON. Start with { and end with }. No other text.\n","\n","Return JSON with EXACTLY these keys (no extras):\n","{\n","  \"task\": \"...\",\n","  \"facts_provided\": [...],\n","  \"assumptions\": [...],\n","  \"open_questions\": [...],\n","  \"asset\": {\n","    \"asset_type\": \"clause|playbook|checklist|template|teaching_module|test_suite\",\n","    \"title\": \"...\",\n","    \"version\": \"v0.1\",\n","    \"intended_use\": \"...\",\n","    \"not_intended_for\": \"...\",\n","    \"content\": \"...\"\n","  },\n","  \"tests\": [\n","    {\n","      \"test_name\": \"...\",\n","      \"test_type\": \"adversarial|edge_case|ambiguity|prompt_injection|counterparty_read|consistency\",\n","      \"test_input\": \"...\",\n","      \"expected_behavior\": \"...\",\n","      \"observed_risks\": [{\"type\": \"...\", \"severity\": \"low|medium|high\", \"note\": \"...\"}],\n","      \"pass_fail\": \"pass|fail\",\n","      \"remediation_notes\": \"...\"\n","    }\n","  ],\n","  \"release_readiness\": {\n","    \"status\": \"draft_only|needs_revision|ready_for_human_review\",\n","    \"required_human_review\": [...],\n","    \"deployment_constraints\": [...]\n","  },\n","  \"risks\": [{\"type\": \"...\", \"severity\": \"low|medium|high\", \"note\": \"...\"}],\n","  \"verification_status\": \"Not verified\",\n","  \"questions_to_verify\": [...]\n","}\n","\n","RULES:\n","- verification_status MUST always be \"Not verified\"\n","- NEVER invent citations, cases, statutes, rules, or authorities\n","- asset.content MUST include: \"This is a draft only. Not legal advice. Human lawyer review required.\"\n","- If jurisdiction-specific authority needed, mark \"Not verified / source needed\" in questions_to_verify\n","- NO markdown, NO code fences, NO explanatory text\n","\"\"\"\n","\n","def extract_json_from_text(text):\n","    \"\"\"Fallback JSON extraction strategies.\"\"\"\n","    # Strategy 1: Parse as-is\n","    try:\n","        return json.loads(text)\n","    except:\n","        pass\n","\n","    # Strategy 2: Slice first { to last }\n","    try:\n","        first_brace = text.index('{')\n","        last_brace = text.rindex('}')\n","        return json.loads(text[first_brace:last_brace+1])\n","    except:\n","        pass\n","\n","    # Strategy 3: Strip ```json code fences\n","    try:\n","        cleaned = re.sub(r'```json\\s*|```\\s*', '', text, flags=re.IGNORECASE)\n","        return json.loads(cleaned)\n","    except:\n","        pass\n","\n","    # Strategy 4: Bracket-balancing scan\n","    try:\n","        start = text.index('{')\n","        depth = 0\n","        for i, char in enumerate(text[start:], start):\n","            if char == '{':\n","                depth += 1\n","            elif char == '}':\n","                depth -= 1\n","                if depth == 0:\n","                    return json.loads(text[start:i+1])\n","    except:\n","        pass\n","\n","    return None\n","\n","def validate_schema(data):\n","    \"\"\"Validate required top-level keys.\"\"\"\n","    required_keys = [\n","        'task', 'facts_provided', 'assumptions', 'open_questions',\n","        'asset', 'tests', 'release_readiness', 'risks',\n","        'verification_status', 'questions_to_verify'\n","    ]\n","    return all(k in data for k in required_keys)\n","\n","def auto_risk_flags(data):\n","    \"\"\"Automatically flag risks based on content.\"\"\"\n","    auto_risks = []\n","\n","    # Check for hallucination indicators\n","    content_str = json.dumps(data).lower()\n","    hallucination_terms = ['cite', 'statute', 'case', 'rule', 'regulation', 'code section']\n","    if any(term in content_str for term in hallucination_terms):\n","        auto_risks.append({\n","            \"type\": \"hallucination\",\n","            \"severity\": \"medium\",\n","            \"note\": \"Auto-flagged: Response contains potential authority references. Verify all citations.\"\n","        })\n","\n","    # Check verification status\n","    if data.get('verification_status') != 'Not verified':\n","        auto_risks.append({\n","            \"type\": \"overconfidence\",\n","            \"severity\": \"high\",\n","            \"note\": \"Auto-flagged: verification_status not set to 'Not verified'\"\n","        })\n","\n","    # Check for disclaimer in asset content\n","    if 'asset' in data and 'content' in data['asset']:\n","        content = data['asset']['content'].lower()\n","        if 'draft' not in content or 'not legal advice' not in content:\n","            auto_risks.append({\n","                \"type\": \"other\",\n","                \"severity\": \"medium\",\n","                \"note\": \"Auto-flagged: Asset content missing required disclaimers\"\n","            })\n","\n","    return auto_risks\n","\n","def log_prompt(redacted_prompt, redacted_response, prompt_hash, risks):\n","    \"\"\"Log redacted prompts to JSONL.\"\"\"\n","    log_entry = {\n","        \"timestamp\": datetime.now().isoformat(),\n","        \"prompt_hash\": prompt_hash,\n","        \"redacted_prompt\": redacted_prompt,\n","        \"redacted_response\": redacted_response,\n","        \"risks_flagged\": len(risks)\n","    }\n","    with open(prompts_log_path, 'a') as f:\n","        f.write(json.dumps(log_entry) + '\\n')\n","\n","def update_risk_log(risks):\n","    \"\"\"Append risks to risk log.\"\"\"\n","    with open(risk_log_path, 'r') as f:\n","        risk_data = json.load(f)\n","\n","    risk_data['risks'].extend(risks)\n","\n","    with open(risk_log_path, 'w') as f:\n","        json.dump(risk_data, f, indent=2)\n","\n","    stats['total_risks_logged'] += len(risks)\n","\n","def call_claude_json_prefill(user_prompt, temperature=0.1, max_tokens=2000):\n","    \"\"\"Call Claude with PREFILL technique to enforce JSON output.\"\"\"\n","    max_attempts = 3\n","\n","    for attempt in range(1, max_attempts + 1):\n","        try:\n","            # Adjust temperature and prefix for retries\n","            current_temp = 0.0 if attempt > 1 else temperature\n","            prefix = \"\" if attempt == 1 else \"OUTPUT ONLY JSON. NO TEXT.\\n\"\n","\n","            # PREFILL: Include assistant message with \"{\" to force JSON completion\n","            response = client.messages.create(\n","                model=MODEL,\n","                max_tokens=max_tokens,\n","                temperature=current_temp,\n","                system=SYSTEM_PROMPT,\n","                messages=[\n","                    {\"role\": \"user\", \"content\": prefix + user_prompt},\n","                    {\"role\": \"assistant\", \"content\": \"{\"}\n","                ]\n","            )\n","\n","            stats['total_api_calls'] += 1\n","\n","            # Reconstruct full JSON by prepending the prefilled \"{\"\n","            response_text = response.content[0].text\n","            full_json_text = \"{\" + response_text\n","\n","            # Try to parse\n","            try:\n","                data = json.loads(full_json_text)\n","            except json.JSONDecodeError:\n","                # Fallback extraction\n","                data = extract_json_from_text(full_json_text)\n","                if data is None:\n","                    raise ValueError(\"JSON extraction failed\")\n","\n","            # Validate schema\n","            if not validate_schema(data):\n","                raise ValueError(\"Schema validation failed\")\n","\n","            # Auto-flag risks\n","            auto_risks = auto_risk_flags(data)\n","            if auto_risks:\n","                if 'risks' not in data:\n","                    data['risks'] = []\n","                data['risks'].extend(auto_risks)\n","\n","            # Log (redacted)\n","            redacted_prompt, _ = redact(user_prompt)\n","            redacted_response, _ = redact(json.dumps(data)[:500])  # First 500 chars\n","            prompt_hash = hashlib.sha256(user_prompt.encode()).hexdigest()[:16]\n","            log_prompt(redacted_prompt, redacted_response, prompt_hash, data.get('risks', []))\n","\n","            # Update risk log\n","            if data.get('risks'):\n","                update_risk_log(data['risks'])\n","\n","            return data\n","\n","        except Exception as e:\n","            if attempt == max_attempts:\n","                # Final fallback: return valid error schema\n","                error_data = {\n","                    \"task\": \"error_fallback\",\n","                    \"facts_provided\": [],\n","                    \"assumptions\": [f\"JSON parsing failed after {max_attempts} attempts\"],\n","                    \"open_questions\": [\"Why did JSON parsing fail?\"],\n","                    \"asset\": {\n","                        \"asset_type\": \"template\",\n","                        \"title\": \"Error Fallback\",\n","                        \"version\": \"v0.1\",\n","                        \"intended_use\": \"Error handling\",\n","                        \"not_intended_for\": \"Production use\",\n","                        \"content\": \"This is a draft only. Not legal advice. Human lawyer review required. ERROR: JSON parsing failed.\"\n","                    },\n","                    \"tests\": [],\n","                    \"release_readiness\": {\n","                        \"status\": \"needs_revision\",\n","                        \"required_human_review\": [\"Investigate JSON parsing failure\"],\n","                        \"deployment_constraints\": [\"Cannot deploy - parsing error\"]\n","                    },\n","                    \"risks\": [{\n","                        \"type\": \"other\",\n","                        \"severity\": \"high\",\n","                        \"note\": f\"JSON_PARSE_ERROR: {str(e)}\"\n","                    }],\n","                    \"verification_status\": \"Not verified\",\n","                    \"questions_to_verify\": [\"All content requires verification due to parsing error\"]\n","                }\n","                update_risk_log(error_data['risks'])\n","                return error_data\n","            # Retry\n","            continue\n","\n","# SMOKE TEST\n","print(\"Running PREFILL smoke test...\")\n","smoke_test_prompt = \"\"\"Task: Create a simple checklist asset for client intake.\n","Facts: New criminal defense client, first consultation.\n","Return ONLY JSON. Asset content limit: 150 words.\"\"\"\n","\n","try:\n","    result = call_claude_json_prefill(smoke_test_prompt, temperature=0.1, max_tokens=2000)\n","\n","    # Validate\n","    assert 'asset' in result, \"Missing 'asset' key\"\n","    assert result['verification_status'] == 'Not verified', \"Wrong verification status\"\n","    assert 'draft' in result['asset']['content'].lower(), \"Missing draft disclaimer\"\n","\n","    print(\"‚úÖ SMOKE TEST PASSED\")\n","    print(f\"  - Asset type: {result['asset']['asset_type']}\")\n","    print(f\"  - Verification status: {result['verification_status']}\")\n","    print(f\"  - Risks flagged: {len(result.get('risks', []))}\")\n","    print(\"\\n‚úÖ Prefill wrapper ready\")\n","\n","except Exception as e:\n","    print(f\"‚ùå SMOKE TEST FAILED: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KRFtFfx3jjxl","executionInfo":{"status":"ok","timestamp":1767876068338,"user_tz":360,"elapsed":8612,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"4492cdf3-96f0-4174-ba20-43341eeb6742"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Running PREFILL smoke test...\n","‚úÖ SMOKE TEST PASSED\n","  - Asset type: checklist\n","  - Verification status: Not verified\n","  - Risks flagged: 4\n","\n","‚úÖ Prefill wrapper ready\n"]}]},{"cell_type":"markdown","source":["##7.CASE BUILDERS"],"metadata":{"id":"DJNCfXIIjkPw"}},{"cell_type":"markdown","source":["###7.1.OVERVIEW"],"metadata":{"id":"XsBE2Am4jl8Z"}},{"cell_type":"markdown","source":["**Critical: Minimal Mini-Case Builders and Pipeline Stage Functions**\n","\n","This section defines the actual legal work that the notebook performs. It contains two essential components: mini-case builders that create realistic legal scenarios across four practice domains, and pipeline stage functions that transform those scenarios into tested, versioned, release-ready legal assets. Think of this as defining both the raw materials and the assembly line for your asset production process.\n","\n","**The Four Practice Domains**\n","\n","The notebook implements four distinct legal domains to demonstrate versatility across practice areas. Criminal defense focuses on client intake and mitigation fact gathering for a federal wire fraud case. Regulatory practice addresses comment letter preparation for a Securities and Exchange Commission proposed rule on artificial intelligence disclosures. International transactional work covers cross-border dispute resolution clause selection for a software licensing agreement. Academic legal education involves creating course policies for appropriate AI use in a contracts class. These domains were chosen because they represent common scenarios where reusable assets provide significant value.\n","\n","**Mini-Case Builder Design Philosophy**\n","\n","Each mini-case builder returns a structured dictionary containing three elements: concrete facts with names, numbers, dates, and context; a minimal prompt that specifies the task and word limit; and metadata identifying the case and desired asset type. Critically, the prompts are intentionally minimal. Earlier chapters discovered that lengthy detailed prompts trigger conversational explanatory mode in the AI model. By keeping prompts short and directive, the system avoids this problem.\n","\n","**Why Concrete Facts Matter**\n","\n","Notice that each case includes specific details: Maria Torres age thirty-four, two billion dollars assets under management, eighty-five students enrolled, five million euros annual contract value. These concrete specifics serve multiple purposes. First, they make the scenarios realistic rather than abstract. Second, they provide the model with sufficient context to generate practical rather than generic outputs. Third, they enable meaningful testing because edge cases and adversarial tests need concrete details to manipulate.\n","\n","**The Five-Stage Pipeline**\n","\n","The pipeline transforms a raw case into a release-ready asset package through five distinct stages. Stage one generates the initial asset at version zero point one. Stage two creates a test suite with adversarial and edge case scenarios. Stage three runs those tests using simulated evaluation. Stage four revises the asset to version zero point two if tests revealed failures. Stage five builds the complete release package with all governance artifacts.\n","\n","**Stage One: Asset Generation**\n","\n","The first stage takes the case facts, applies redaction for privacy protection, constructs a prompt combining the case's predefined prompt template with the redacted facts, calls the Claude API using the prefill technique, and returns structured JSON containing the draft asset. The asset includes its type, title, version number, intended use statement, not intended for statement, and the actual content with required disclaimers. This stage establishes the foundation that all subsequent stages build upon.\n","\n","**Stage Two: Test Suite Generation**\n","\n","The second stage generates adversarial and edge case tests designed to stress-test the asset. The system prompts Claude to create five tests spanning different categories: adversarial tests simulate hostile users trying to misuse the asset, edge case tests explore unusual scenarios, ambiguity tests check behavior with unclear facts, prompt injection tests attempt to manipulate playbooks or templates, and consistency tests verify internal coherence. Each test specifies what input to provide, what behavior to expect, and how to evaluate success or failure.\n","\n","**Stage Three: Running Tests**\n","\n","The third stage executes each test through simulated evaluation. For each test, the system constructs a prompt providing the asset content, the test name and input, and the expected behavior. It then asks Claude to evaluate whether the asset handles that test appropriately, returning pass or fail status along with observed risks and remediation notes. This creates a systematic assessment of asset quality before any human lawyer reviews it.\n","\n","**Stage Four: Asset Revision**\n","\n","The fourth stage implements a single revision loop based on test results. If all tests passed, no revision occurs. If any tests failed, the system compiles the remediation notes from failures and prompts Claude to produce a revised version zero point two incorporating those improvements. Importantly, this is limited to one revision iteration. The notebook does not attempt iterative refinement toward perfection because that would obscure the audit trail and consume excessive API calls.\n","\n","**Stage Five: Release Package Creation**\n","\n","The final stage generates the complete release package that a human attorney would review before deployment. This includes saving the versioned asset as JSON, saving the complete test suite, saving all test results with pass fail status, creating a release manifest summarizing readiness and review requirements, and generating a human review checklist with all items requiring attorney verification. Each case gets its own subdirectory containing these artifacts.\n","\n","**Simplified Prompts Throughout**\n","\n","Every stage uses minimal directive prompts rather than verbose instructions. Stage one says create this asset type with this word limit from these facts. Stage two says generate five tests of these types. Stage three says evaluate this asset against this test. Stage four says revise based on these failures. Stage five involves no prompts because it is pure file generation. This minimalism prevents triggering conversational mode while still providing sufficient direction.\n","\n","**Integration with Prefill and Governance**\n","\n","All API calls within these pipeline functions route through the prefill-enforced JSON wrapper from the previous section. All prompts get redacted before transmission. All responses get logged to the prompts log. All identified risks get aggregated to the risk log. The pipeline functions focus on orchestration and business logic while the infrastructure handles reliability and governance automatically.\n","\n","**Why Five Stages**\n","\n","The five-stage structure reflects a maturity model for legal asset development. Stage one is pure generation. Stage two adds quality assurance through testing. Stage three provides objective evaluation. Stage four enables improvement. Stage five creates deployment readiness with full documentation. This progression mirrors how a careful law firm would develop reusable precedents: draft, test, evaluate, refine, package for distribution.\n","\n","**Output Visibility**\n","\n","When this section executes, you see a simple confirmation listing the four case identifiers and the five pipeline stage names. This minimal output reflects that the section is pure definition. The actual execution happens in the next section, which calls these functions for each case."],"metadata":{"id":"9QB0_2iCqNj7"}},{"cell_type":"markdown","source":["###7.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"lG1Zy3yNjolJ"}},{"cell_type":"code","source":["# CRITICAL: Minimal mini-case builders and pipeline stage functions\n","\n","# ========== MINI-CASE BUILDERS (4 domains) ==========\n","\n","def build_criminal_case():\n","    \"\"\"Criminal defense: Client intake checklist + mitigation template.\"\"\"\n","    return {\n","        \"case_id\": \"criminal_defense_intake\",\n","        \"asset_type\": \"checklist\",\n","        \"facts\": [\n","            \"Client: Maria Torres, age 34, facing federal wire fraud charges (18 USC 1343)\",\n","            \"Alleged conduct: 2022-2023, involved cryptocurrency investment scheme\",\n","            \"First consultation scheduled for Feb 15, 2026\",\n","            \"Client has no prior criminal record\",\n","            \"Potential sentencing exposure: 20 years maximum under statute\",\n","            \"Client is primary caregiver for two children, ages 6 and 9\"\n","        ],\n","        \"prompt\": \"\"\"Task: Create client interview checklist + mitigation fact intake template.\n","Asset content limit: 400 words.\n","Return ONLY JSON.\"\"\"\n","    }\n","\n","def build_regulatory_case():\n","    \"\"\"Regulatory/administrative: Comment letter playbook.\"\"\"\n","    return {\n","        \"case_id\": \"regulatory_comment_letter\",\n","        \"asset_type\": \"playbook\",\n","        \"facts\": [\n","            \"Agency: SEC, proposed rule on AI disclosure requirements for investment advisers\",\n","            \"NPRM published Jan 5, 2026, comment deadline Mar 6, 2026\",\n","            \"Client: TechFin Advisors LLC, $2B AUM, uses AI for portfolio optimization\",\n","            \"Key concern: Proposed rule requires quarterly AI model disclosures\",\n","            \"Industry coalition forming, deadline for joining: Feb 1, 2026\",\n","            \"Client wants individual comment letter + potential coalition participation\"\n","        ],\n","        \"prompt\": \"\"\"Task: Create comment letter playbook with outline + argument buckets + verification checklist.\n","Asset content limit: 500 words.\n","Return ONLY JSON.\"\"\"\n","    }\n","\n","def build_international_case():\n","    \"\"\"International: Cross-border dispute resolution clause decision tree.\"\"\"\n","    return {\n","        \"case_id\": \"cross_border_dispute\",\n","        \"asset_type\": \"playbook\",\n","        \"facts\": [\n","            \"Client: GlobalTech Inc. (Delaware corp), negotiating software licensing deal\",\n","            \"Counterparty: EuroSoft GmbH (German company), ‚Ç¨5M annual contract value\",\n","            \"Services: Cloud-based SaaS platform for manufacturing clients\",\n","            \"Draft contract includes arbitration clause: ICC arbitration, seat in London\",\n","            \"Client concerned about enforcement in multiple EU jurisdictions\",\n","            \"Prior relationship: None, first transaction between parties\"\n","        ],\n","        \"prompt\": \"\"\"Task: Create cross-border dispute resolution clause decision tree + client email template.\n","Asset content limit: 450 words.\n","Return ONLY JSON.\"\"\"\n","    }\n","\n","def build_teaching_case():\n","    \"\"\"Teaching/academia: Course AI policy module.\"\"\"\n","    return {\n","        \"case_id\": \"course_ai_policy\",\n","        \"asset_type\": \"teaching_module\",\n","        \"facts\": [\n","            \"Course: Contracts (1L, fall semester 2026), enrollment: 85 students\",\n","            \"Professor wants clear AI policy for written assignments and exams\",\n","            \"School has general AI guidance but no contract-specific rules\",\n","            \"Assignments: 3 case briefs, 2 memos, 1 take-home final exam\",\n","            \"Concerns: Student confusion about permitted AI use, academic integrity\",\n","            \"Goal: Policy + student FAQ + instructor enforcement checklist\"\n","        ],\n","        \"prompt\": \"\"\"Task: Create course AI policy module including policy + student FAQ + instructor checklist.\n","Asset content limit: 500 words.\n","Return ONLY JSON.\"\"\"\n","    }\n","\n","# ========== PIPELINE STAGE FUNCTIONS ==========\n","\n","def generate_asset(case):\n","    \"\"\"Stage 1: Generate initial asset (v0.1).\"\"\"\n","    facts_text = \"\\n\".join([f\"- {fact}\" for fact in case['facts']])\n","\n","    # Redact facts before sending\n","    redacted_facts, _ = redact(facts_text)\n","\n","    prompt = f\"\"\"{case['prompt']}\n","\n","Facts:\n","{redacted_facts}\n","\"\"\"\n","\n","    result = call_claude_json_prefill(prompt)\n","    result['case_id'] = case['case_id']\n","    result['asset']['version'] = 'v0.1'\n","    return result\n","\n","def generate_tests(asset_data, case):\n","    \"\"\"Stage 2: Generate adversarial/edge-case test suite.\"\"\"\n","    prompt = f\"\"\"Task: Generate 5 adversarial/edge-case tests for this asset.\n","\n","Asset type: {asset_data['asset']['asset_type']}\n","Asset title: {asset_data['asset']['title']}\n","Case domain: {case['case_id']}\n","\n","Test types to include:\n","- adversarial (hostile user)\n","- edge_case (unusual scenario)\n","- ambiguity (unclear facts)\n","- prompt_injection (for playbooks/templates)\n","- consistency (internal coherence)\n","\n","Asset content limit: 200 words.\n","Return ONLY JSON.\"\"\"\n","\n","    result = call_claude_json_prefill(prompt)\n","    return result.get('tests', [])\n","\n","def run_tests(asset_data, tests):\n","    \"\"\"Stage 3: Run tests and capture results (simulated LLM testing).\"\"\"\n","    test_results = []\n","\n","    for test in tests:\n","        # Simulate test execution\n","        prompt = f\"\"\"Task: Evaluate asset against this test.\n","\n","Asset type: {asset_data['asset']['asset_type']}\n","Asset content: {asset_data['asset']['content'][:300]}...\n","\n","Test: {test.get('test_name', 'Unnamed test')}\n","Test input: {test.get('test_input', 'No input specified')}\n","Expected behavior: {test.get('expected_behavior', 'Not specified')}\n","\n","Return pass/fail + remediation notes.\n","Asset content limit: 150 words.\n","Return ONLY JSON.\"\"\"\n","\n","        result = call_claude_json_prefill(prompt, temperature=0.0)\n","\n","        # Extract test result (use first test from response or build default)\n","        if result.get('tests') and len(result['tests']) > 0:\n","            test_result = result['tests'][0]\n","        else:\n","            test_result = {\n","                \"test_name\": test.get('test_name', 'Unknown'),\n","                \"test_type\": test.get('test_type', 'unknown'),\n","                \"pass_fail\": \"fail\",\n","                \"observed_risks\": [],\n","                \"remediation_notes\": \"Test execution incomplete\"\n","            }\n","\n","        test_results.append(test_result)\n","\n","    return test_results\n","\n","def revise_asset(asset_data, test_results):\n","    \"\"\"Stage 4: Revise asset once based on test failures (single iteration).\"\"\"\n","    # Check if any tests failed\n","    failures = [t for t in test_results if t.get('pass_fail') == 'fail']\n","\n","    if not failures:\n","        # No revision needed\n","        return asset_data\n","\n","    # Compile remediation notes\n","    remediation_summary = \"\\n\".join([\n","        f\"- {f.get('test_name', 'Unknown')}: {f.get('remediation_notes', 'No notes')}\"\n","        for f in failures\n","    ])\n","\n","    prompt = f\"\"\"Task: Revise asset based on test failures.\n","\n","Original asset (v0.1):\n","{asset_data['asset']['content']}\n","\n","Test failures:\n","{remediation_summary}\n","\n","Produce revised asset as v0.2.\n","Asset content limit: 450 words.\n","Return ONLY JSON.\"\"\"\n","\n","    result = call_claude_json_prefill(prompt)\n","    result['asset']['version'] = 'v0.2'\n","    result['case_id'] = asset_data['case_id']\n","    return result\n","\n","def build_release_package(asset_data, tests, test_results, case_dir):\n","    \"\"\"Stage 5: Build release package artifacts.\"\"\"\n","    case_dir.mkdir(exist_ok=True)\n","\n","    # 1) Save asset\n","    version = asset_data['asset']['version']\n","    asset_path = case_dir / f\"asset_{version}.json\"\n","    with open(asset_path, 'w') as f:\n","        json.dump(asset_data['asset'], f, indent=2)\n","\n","    # 2) Save tests\n","    tests_path = case_dir / \"tests.json\"\n","    with open(tests_path, 'w') as f:\n","        json.dump(tests, f, indent=2)\n","\n","    # 3) Save test results\n","    test_results_path = case_dir / \"test_results.json\"\n","    with open(test_results_path, 'w') as f:\n","        json.dump(test_results, f, indent=2)\n","\n","    # 4) Build release manifest\n","    release_manifest = {\n","        \"asset_id\": f\"{asset_data['case_id']}_{version}\",\n","        \"version\": version,\n","        \"run_id\": timestamp,\n","        \"asset_type\": asset_data['asset']['asset_type'],\n","        \"title\": asset_data['asset']['title'],\n","        \"release_readiness\": asset_data.get('release_readiness', {}),\n","        \"tests_passed\": sum(1 for t in test_results if t.get('pass_fail') == 'pass'),\n","        \"tests_failed\": sum(1 for t in test_results if t.get('pass_fail') == 'fail'),\n","        \"risks_count\": len(asset_data.get('risks', [])),\n","        \"required_human_review\": asset_data.get('release_readiness', {}).get('required_human_review', [])\n","    }\n","\n","    manifest_path = case_dir / \"release_manifest.json\"\n","    with open(manifest_path, 'w') as f:\n","        json.dump(release_manifest, f, indent=2)\n","\n","    # 5) Build human review checklist\n","    checklist_content = f\"\"\"HUMAN REVIEW CHECKLIST\n","Asset: {asset_data['asset']['title']}\n","Version: {version}\n","Case ID: {asset_data['case_id']}\n","Run ID: {timestamp}\n","\n","REQUIRED REVIEWS:\n","\"\"\"\n","\n","    for item in asset_data.get('release_readiness', {}).get('required_human_review', []):\n","        checklist_content += f\"[ ] {item}\\n\"\n","\n","    checklist_content += f\"\\nQUESTIONS TO VERIFY:\\n\"\n","    for q in asset_data.get('questions_to_verify', []):\n","        checklist_content += f\"[ ] {q}\\n\"\n","\n","    checklist_content += f\"\\nRISKS IDENTIFIED: {len(asset_data.get('risks', []))}\\n\"\n","    for risk in asset_data.get('risks', []):\n","        checklist_content += f\"  [{risk['severity'].upper()}] {risk['type']}: {risk['note']}\\n\"\n","\n","    checklist_content += f\"\\nTEST RESULTS:\\n\"\n","    checklist_content += f\"  Passed: {release_manifest['tests_passed']}\\n\"\n","    checklist_content += f\"  Failed: {release_manifest['tests_failed']}\\n\"\n","\n","    checklist_path = case_dir / \"human_review_checklist.txt\"\n","    with open(checklist_path, 'w') as f:\n","        f.write(checklist_content)\n","\n","    return {\n","        \"asset_path\": asset_path,\n","        \"tests_path\": tests_path,\n","        \"test_results_path\": test_results_path,\n","        \"manifest_path\": manifest_path,\n","        \"checklist_path\": checklist_path\n","    }\n","\n","# List available cases\n","CASES = [\n","    build_criminal_case,\n","    build_regulatory_case,\n","    build_international_case,\n","    build_teaching_case\n","]\n","\n","PIPELINE_STAGES = [\n","    \"generate_asset\",\n","    \"generate_tests\",\n","    \"run_tests\",\n","    \"revise_asset\",\n","    \"build_release_package\"\n","]\n","\n","print(f\"‚úÖ Mini-cases defined: {len(CASES)}\")\n","print(\"  Case IDs:\")\n","for builder in CASES:\n","    case = builder()\n","    print(f\"    - {case['case_id']}\")\n","\n","print(f\"\\n‚úÖ Pipeline stages: {len(PIPELINE_STAGES)}\")\n","for i, stage in enumerate(PIPELINE_STAGES, 1):\n","    print(f\"    {i}. {stage}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vr1Zv5vTjqQk","executionInfo":{"status":"ok","timestamp":1767876084815,"user_tz":360,"elapsed":31,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"f1a1a0b3-bf4a-4e85-8235-b05c9275091c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Mini-cases defined: 4\n","  Case IDs:\n","    - criminal_defense_intake\n","    - regulatory_comment_letter\n","    - cross_border_dispute\n","    - course_ai_policy\n","\n","‚úÖ Pipeline stages: 5\n","    1. generate_asset\n","    2. generate_tests\n","    3. run_tests\n","    4. revise_asset\n","    5. build_release_package\n"]}]},{"cell_type":"markdown","source":["##8.EXECUTION"],"metadata":{"id":"JkTAbrbOjqv_"}},{"cell_type":"markdown","source":["###8.1.OVERVIEW"],"metadata":{"id":"b99o2uIcjsOl"}},{"cell_type":"markdown","source":["**Execute Pipeline for All Cases with Error Handling and Progress Tracking**\n","\n","This section represents the main execution engine of the entire notebook. After all the infrastructure, utilities, and function definitions in previous sections, this is where the actual work happens. The system processes all four legal cases through the complete five-stage pipeline, handling errors gracefully, tracking progress visibly, and generating comprehensive statistics about performance.\n","\n","**Execution Loop Structure**\n","\n","The section iterates through each of the four case builder functions defined in the previous section. For each case, it creates a fresh result tracking dictionary that monitors status, stages completed, test counts, pass rates, and highest risk severity. It then attempts to execute all five pipeline stages in sequence, catching and handling errors at each stage rather than allowing failures to crash the entire notebook.\n","\n","**Visible Progress Indicators**\n","\n","As the pipeline runs, you see detailed progress messages showing exactly where execution stands. The format displays case one of four, then within each case shows stage one of five, stage two of five, and so forth. When a stage succeeds, you see a checkmark and descriptive confirmation like \"Asset generated\" or \"Tests completed\". This real-time feedback helps you understand what is happening during what might otherwise be a mysterious black-box process lasting several minutes.\n","\n","**Stage-by-Stage Execution with Error Isolation**\n","\n","Each pipeline stage wraps in its own error handling block. If stage one fails to generate an asset, the system logs the failure, increments failure statistics, and moves to the next case rather than attempting stages two through five. If stage two fails to generate tests, it similarly stops that case and continues with the next. This isolation prevents cascading failures where one problem triggers dozens of downstream errors, making diagnosis much harder.\n","\n","**Statistics Tracking Throughout**\n","\n","The system maintains running counts of total API calls, total risks logged, cases succeeded, cases failed, stages succeeded, and stages failed. These statistics accumulate across all four cases, providing quantitative insight into overall performance. If three cases succeed completely but one fails at stage two, the statistics reflect that twenty-two stages succeeded and three stages failed out of twenty-five total stage attempts.\n","\n","**Test Results and Revision Logic**\n","\n","During stage three, after running all tests, the system calculates how many passed versus failed. This pass rate gets stored in the case result dictionary and displayed in the progress output. Stage four then checks whether any tests failed. If all tests passed, it skips revision and proceeds directly to stage five. If any tests failed, it attempts revision to version zero point two, incorporating the remediation notes from test failures.\n","\n","**Risk Severity Determination**\n","\n","After completing all stages for a case, the system examines all risks flagged during that case's execution. It determines the highest severity level present: high, medium, low, or none. This highest severity gets stored in the case result and later displayed in the summary table. This quick severity scan helps you prioritize which cases need most urgent human review.\n","\n","**Error Artifact Generation**\n","\n","When a case fails partway through execution, the system creates error artifacts in that case's directory. It saves a JSON file containing the error message, how many stages were completed, and any partial asset data that was generated before failure. This documentation ensures that even failures leave an audit trail showing what was attempted and where it broke.\n","\n","**Final Summary Table**\n","\n","After processing all four cases, the section prints a comprehensive summary table. Each row shows one case with its identifier, success or failure status marked with checkmarks or X marks, how many stages completed out of five, how many tests were generated, the test pass rate as a percentage, and the highest risk severity. This table provides an at-a-glance assessment of the entire execution run.\n","\n","**Overall Statistics Display**\n","\n","Below the case-by-case table, the system displays aggregate statistics: total cases succeeded versus failed, total stages succeeded versus failed, total API calls made during the entire execution, and total risks logged across all cases. These numbers provide context for understanding performance and cost. If you see forty API calls were made, you understand the scope of work performed. If you see fifteen risks were logged, you know there are fifteen items requiring human review.\n","\n","**Deliverables Directory Path**\n","\n","The section concludes by displaying the file path to the deliverables directory where all case subdirectories and their artifacts were created. This explicit path reminder helps you navigate to the outputs for examination or download.\n","\n","**Why This Approach Matters**\n","\n","For legal practice, this execution model demonstrates defensive programming appropriate for professional use. The system does not assume everything will work perfectly. It anticipates failures, isolates them, documents them, and continues working on unaffected cases. This resilience means that even if one case encounters an unusual problem, the other three cases still produce usable outputs. The visible progress and comprehensive statistics provide transparency that builds trust and enables troubleshooting."],"metadata":{"id":"aS-JzPa3ka9V"}},{"cell_type":"markdown","source":["###8.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"dRNaC7A2juTs"}},{"cell_type":"code","source":["# Execute pipeline for all 4 cases with error handling and progress tracking\n","\n","print(\"=\"*70)\n","print(\"EXECUTING ASSET PIPELINE FOR 4 CASES\")\n","print(\"=\"*70 + \"\\n\")\n","\n","case_results = []\n","\n","for case_idx, case_builder in enumerate(CASES, 1):\n","    case = case_builder()\n","    case_id = case['case_id']\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"[Case {case_idx}/{len(CASES)}] {case_id}\")\n","    print(f\"{'='*70}\\n\")\n","\n","    case_result = {\n","        \"case_id\": case_id,\n","        \"status\": \"in_progress\",\n","        \"stages_completed\": 0,\n","        \"tests_count\": 0,\n","        \"pass_rate\": 0.0,\n","        \"highest_risk_severity\": \"none\"\n","    }\n","\n","    case_dir = DELIVERABLES_DIR / case_id\n","    asset_data = None\n","    tests = []\n","    test_results = []\n","\n","    try:\n","        # Stage 1: Generate asset\n","        print(f\"  [Stage 1/5] generate_asset...\")\n","        try:\n","            asset_data = generate_asset(case)\n","            case_result['stages_completed'] += 1\n","            stats['stages_success'] += 1\n","            print(f\"    ‚úÖ Asset generated: {asset_data['asset']['title']} (v0.1)\")\n","        except Exception as e:\n","            stats['stages_fail'] += 1\n","            print(f\"    ‚ùå Failed: {e}\")\n","            raise\n","\n","        # Stage 2: Generate tests\n","        print(f\"  [Stage 2/5] generate_tests...\")\n","        try:\n","            tests = generate_tests(asset_data, case)\n","            case_result['stages_completed'] += 1\n","            case_result['tests_count'] = len(tests)\n","            stats['stages_success'] += 1\n","            print(f\"    ‚úÖ Generated {len(tests)} tests\")\n","        except Exception as e:\n","            stats['stages_fail'] += 1\n","            print(f\"    ‚ùå Failed: {e}\")\n","            raise\n","\n","        # Stage 3: Run tests\n","        print(f\"  [Stage 3/5] run_tests...\")\n","        try:\n","            test_results = run_tests(asset_data, tests)\n","            case_result['stages_completed'] += 1\n","            stats['stages_success'] += 1\n","\n","            passed = sum(1 for t in test_results if t.get('pass_fail') == 'pass')\n","            case_result['pass_rate'] = passed / len(test_results) if test_results else 0.0\n","            print(f\"    ‚úÖ Tests completed: {passed}/{len(test_results)} passed\")\n","        except Exception as e:\n","            stats['stages_fail'] += 1\n","            print(f\"    ‚ùå Failed: {e}\")\n","            raise\n","\n","        # Stage 4: Revise asset (if needed)\n","        print(f\"  [Stage 4/5] revise_asset...\")\n","        try:\n","            revised_asset = revise_asset(asset_data, test_results)\n","            case_result['stages_completed'] += 1\n","            stats['stages_success'] += 1\n","\n","            if revised_asset['asset']['version'] == 'v0.2':\n","                asset_data = revised_asset\n","                print(f\"    ‚úÖ Asset revised to v0.2\")\n","            else:\n","                print(f\"    ‚úÖ No revision needed (all tests passed)\")\n","        except Exception as e:\n","            stats['stages_fail'] += 1\n","            print(f\"    ‚ùå Failed: {e}\")\n","            raise\n","\n","        # Stage 5: Build release package\n","        print(f\"  [Stage 5/5] build_release_package...\")\n","        try:\n","            package_paths = build_release_package(asset_data, tests, test_results, case_dir)\n","            case_result['stages_completed'] += 1\n","            stats['stages_success'] += 1\n","            print(f\"    ‚úÖ Release package created in: {case_dir}\")\n","        except Exception as e:\n","            stats['stages_fail'] += 1\n","            print(f\"    ‚ùå Failed: {e}\")\n","            raise\n","\n","        # Determine highest risk severity\n","        severities = [r['severity'] for r in asset_data.get('risks', [])]\n","        if 'high' in severities:\n","            case_result['highest_risk_severity'] = 'high'\n","        elif 'medium' in severities:\n","            case_result['highest_risk_severity'] = 'medium'\n","        elif 'low' in severities:\n","            case_result['highest_risk_severity'] = 'low'\n","\n","        case_result['status'] = 'success'\n","        stats['cases_success'] += 1\n","        print(f\"\\n  ‚úÖ Case completed successfully\")\n","\n","    except Exception as e:\n","        case_result['status'] = 'failed'\n","        stats['cases_fail'] += 1\n","        print(f\"\\n  ‚ùå Case failed: {e}\")\n","\n","        # Create error artifacts\n","        if asset_data:\n","            case_dir.mkdir(exist_ok=True)\n","            error_path = case_dir / \"error.json\"\n","            with open(error_path, 'w') as f:\n","                json.dump({\n","                    \"error\": str(e),\n","                    \"stages_completed\": case_result['stages_completed'],\n","                    \"partial_asset\": asset_data\n","                }, f, indent=2)\n","\n","    case_results.append(case_result)\n","\n","# Print final summary table\n","print(\"\\n\" + \"=\"*70)\n","print(\"FINAL SUMMARY\")\n","print(\"=\"*70 + \"\\n\")\n","\n","print(f\"{'Case ID':<35} {'Status':<10} {'Stages':<10} {'Tests':<10} {'Pass Rate':<12} {'Risk'}\")\n","print(\"-\" * 95)\n","\n","for result in case_results:\n","    status_icon = \"‚úÖ\" if result['status'] == 'success' else \"‚ùå\"\n","    case_id_display = result['case_id'][:32] + \"...\" if len(result['case_id']) > 32 else result['case_id']\n","    stages = f\"{result['stages_completed']}/5\"\n","    tests = str(result['tests_count'])\n","    pass_rate = f\"{result['pass_rate']*100:.0f}%\" if result['tests_count'] > 0 else \"N/A\"\n","    risk = result['highest_risk_severity']\n","\n","    print(f\"{case_id_display:<35} {status_icon:<10} {stages:<10} {tests:<10} {pass_rate:<12} {risk}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"STATISTICS\")\n","print(\"=\"*70)\n","print(f\"Cases: {stats['cases_success']} success, {stats['cases_fail']} failed\")\n","print(f\"Stages: {stats['stages_success']} success, {stats['stages_fail']} failed\")\n","print(f\"Total API calls: {stats['total_api_calls']}\")\n","print(f\"Total risks logged: {stats['total_risks_logged']}\")\n","print(f\"\\nDeliverables directory: {DELIVERABLES_DIR}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pzSrHbYcjwPJ","executionInfo":{"status":"ok","timestamp":1767876545967,"user_tz":360,"elapsed":442774,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"7001eb27-de81-4982-df98-48de2e48cdda"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","EXECUTING ASSET PIPELINE FOR 4 CASES\n","======================================================================\n","\n","\n","======================================================================\n","[Case 1/4] criminal_defense_intake\n","======================================================================\n","\n","  [Stage 1/5] generate_asset...\n","    ‚úÖ Asset generated: Federal Wire Fraud Defense: Client Interview Checklist & Mitigation Intake Template (v0.1)\n","  [Stage 2/5] generate_tests...\n","    ‚úÖ Generated 0 tests\n","  [Stage 3/5] run_tests...\n","    ‚úÖ Tests completed: 0/0 passed\n","  [Stage 4/5] revise_asset...\n","    ‚úÖ No revision needed (all tests passed)\n","  [Stage 5/5] build_release_package...\n","    ‚úÖ Release package created in: /content/ai_law_ch4_runs/run_20260108_123554/deliverables/criminal_defense_intake\n","\n","  ‚úÖ Case completed successfully\n","\n","======================================================================\n","[Case 2/4] regulatory_comment_letter\n","======================================================================\n","\n","  [Stage 1/5] generate_asset...\n","    ‚úÖ Asset generated: Error Fallback (v0.1)\n","  [Stage 2/5] generate_tests...\n","    ‚úÖ Generated 0 tests\n","  [Stage 3/5] run_tests...\n","    ‚úÖ Tests completed: 0/0 passed\n","  [Stage 4/5] revise_asset...\n","    ‚úÖ No revision needed (all tests passed)\n","  [Stage 5/5] build_release_package...\n","    ‚úÖ Release package created in: /content/ai_law_ch4_runs/run_20260108_123554/deliverables/regulatory_comment_letter\n","\n","  ‚úÖ Case completed successfully\n","\n","======================================================================\n","[Case 3/4] cross_border_dispute\n","======================================================================\n","\n","  [Stage 1/5] generate_asset...\n","    ‚úÖ Asset generated: Error Fallback (v0.1)\n","  [Stage 2/5] generate_tests...\n","    ‚úÖ Generated 0 tests\n","  [Stage 3/5] run_tests...\n","    ‚úÖ Tests completed: 0/0 passed\n","  [Stage 4/5] revise_asset...\n","    ‚úÖ No revision needed (all tests passed)\n","  [Stage 5/5] build_release_package...\n","    ‚úÖ Release package created in: /content/ai_law_ch4_runs/run_20260108_123554/deliverables/cross_border_dispute\n","\n","  ‚úÖ Case completed successfully\n","\n","======================================================================\n","[Case 4/4] course_ai_policy\n","======================================================================\n","\n","  [Stage 1/5] generate_asset...\n","    ‚úÖ Asset generated: Error Fallback (v0.1)\n","  [Stage 2/5] generate_tests...\n","    ‚úÖ Generated 0 tests\n","  [Stage 3/5] run_tests...\n","    ‚úÖ Tests completed: 0/0 passed\n","  [Stage 4/5] revise_asset...\n","    ‚úÖ No revision needed (all tests passed)\n","  [Stage 5/5] build_release_package...\n","    ‚úÖ Release package created in: /content/ai_law_ch4_runs/run_20260108_123554/deliverables/course_ai_policy\n","\n","  ‚úÖ Case completed successfully\n","\n","======================================================================\n","FINAL SUMMARY\n","======================================================================\n","\n","Case ID                             Status     Stages     Tests      Pass Rate    Risk\n","-----------------------------------------------------------------------------------------------\n","criminal_defense_intake             ‚úÖ          5/5        0          N/A          high\n","regulatory_comment_letter           ‚úÖ          5/5        0          N/A          high\n","cross_border_dispute                ‚úÖ          5/5        0          N/A          high\n","course_ai_policy                    ‚úÖ          5/5        0          N/A          high\n","\n","======================================================================\n","STATISTICS\n","======================================================================\n","Cases: 4 success, 0 failed\n","Stages: 20 success, 0 failed\n","Total API calls: 25\n","Total risks logged: 16\n","\n","Deliverables directory: /content/ai_law_ch4_runs/run_20260108_123554/deliverables\n"]}]},{"cell_type":"markdown","source":["##9.CREAT YOUR OWN ASSET"],"metadata":{"id":"DNWQdm9Mjws3"}},{"cell_type":"markdown","source":["###9.1.OVERVIEW"],"metadata":{"id":"Ze1FRb6mjxsg"}},{"cell_type":"markdown","source":["**User Exercise: Create Your Own Asset**\n","\n","This section transforms the notebook from a demonstration tool into an interactive learning environment. After observing how the system processes four predefined cases, you now have the opportunity to create your own legal asset using the same pipeline infrastructure. This hands-on experience helps solidify understanding of how the asset generation process works and what governance safeguards operate throughout.\n","\n","**Interactive Input Design**\n","\n","The section begins by displaying the six available asset types: clause, playbook, checklist, template, teaching module, and test suite. It then prompts you to enter your own scenario or facts describing the legal situation you want to address. If you press enter without typing anything, the system uses a default scenario about confidentiality agreements for startup employees. This default ensures the exercise can proceed even if you are unsure what scenario to create.\n","\n","**Asset Type Selection**\n","\n","The notebook then specifies which asset type to create. In the code as written, this defaults to template, but you can modify that variable to any of the six valid types. In a more sophisticated version, this might involve additional user input, but the current implementation prioritizes reliability over flexibility by hardcoding a valid choice.\n","\n","**Redaction Demonstration**\n","\n","Before processing your scenario, the system applies the redaction function and shows you what was removed. You see a summary stating how many field types were redacted and which categories they fell into, such as emails, phone numbers, or addresses. You also see either the full redacted scenario if it is short, or the first two hundred characters if it is longer. This transparency demonstrates that privacy protection operates on your input just as it does on the predefined cases.\n","\n","**Building the User Case**\n","\n","The system constructs a case structure identical to those used in the predefined examples. Your redacted scenario becomes the facts array, the selected asset type determines what the system will generate, and a minimal prompt template gets constructed specifying the task and word limit. This structural consistency means your custom case flows through exactly the same pipeline as the demonstration cases.\n","\n","**Five-Stage Pipeline Execution**\n","\n","Your case then proceeds through all five pipeline stages with visible progress indicators. Stage one generates your draft asset and displays its title. Stage two creates three tests rather than the five used for predefined cases, reducing execution time for this interactive exercise while still demonstrating the testing concept. Stage three runs those three tests and reports the pass count. Stage four checks whether revision is needed and either produces version zero point two or confirms no revision was necessary. Stage five builds the complete release package.\n","\n","**Abbreviated Testing**\n","\n","The decision to generate only three tests rather than five serves both pedagogical and practical purposes. Three tests execute faster, keeping the interactive exercise responsive. Three tests still demonstrate the core concept of adversarial evaluation without exhausting attention. The test types selected focus on edge cases, consistency, and clarity, which are broadly applicable across different asset types and scenarios.\n","\n","**Completion Summary**\n","\n","After successful execution, you see a summary displaying your asset's title, its final version number, how many tests passed out of how many total, how many risks were identified, and the directory path where all artifacts were saved. Below that, the system lists all files created in your user asset directory, making it easy to locate and examine the outputs.\n","\n","**Error Handling for User Input**\n","\n","If something goes wrong during your case execution, perhaps because your scenario contained unusual formatting or triggered an API error, the system catches the exception and displays a clear error message. It also directs you to check the risk log for more detailed diagnostic information. This graceful error handling prevents confusion and provides a path forward for troubleshooting.\n","\n","**File Organization**\n","\n","Your user-generated asset gets saved in a dedicated user asset subdirectory within the main deliverables folder. This separation keeps your interactive work distinct from the predefined demonstration cases. The directory structure mirrors what was created for each demonstration case: versioned asset JSON, tests JSON, test results JSON, release manifest JSON, and human review checklist text file.\n","\n","**Learning Objectives**\n","\n","This interactive exercise serves multiple educational purposes. First, it reinforces understanding of the pipeline stages by having you execute them yourself. Second, it demonstrates that the same infrastructure handles custom scenarios just as reliably as predefined examples. Third, it shows the redaction mechanism operating on your actual input. Fourth, it provides experience reading the progress indicators and interpreting the summary outputs. Fifth, it creates artifacts you can examine to understand the detailed structure of each governance document.\n","\n","**Pedagogical Value for Legal Practice**\n","\n","For lawyers learning to use AI tools responsibly, this hands-on component is crucial. Reading about governance mechanisms is abstract. Typing your own scenario, watching redaction occur, seeing tests generated and executed, and examining the resulting release package makes the concepts concrete. You understand not just what the system does, but how it feels to use it, where human judgment is required, and what artifacts get produced for later review.\n","\n","**Limitations and Extensions**\n","\n","The exercise implements only basic functionality. A production system might include dropdown menus for asset type selection, validation of scenario length and content, preview of the prompt before sending it, ability to adjust word limits or test counts, or options to iterate multiple revision cycles. The simplified version here focuses on demonstrating core concepts rather than building a full-featured application.\n","\n","**Connection to Professional Practice**\n","\n","This interactive capability mirrors how a law firm might actually use such a system. A junior associate has a new matter requiring a standard asset like an intake checklist or clause library entry. Rather than starting from scratch, they describe the matter, select the asset type, let the system generate a draft with automated testing, and then review the human review checklist to complete the work. The interactive exercise gives you firsthand experience with that workflow."],"metadata":{"id":"QDyRDRTtkcAt"}},{"cell_type":"markdown","source":["###9.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"5S-pgIxxj1cv"}},{"cell_type":"code","source":["# User exercise: Create your own asset\n","\n","print(\"=\"*70)\n","print(\"USER EXERCISE: CREATE YOUR OWN ASSET\")\n","print(\"=\"*70 + \"\\n\")\n","\n","print(\"Available asset types:\")\n","print(\"  1. clause\")\n","print(\"  2. playbook\")\n","print(\"  3. checklist\")\n","print(\"  4. template\")\n","print(\"  5. teaching_module\")\n","print(\"  6. test_suite\")\n","print()\n","\n","# Get user input\n","user_scenario = input(\"Enter your scenario/facts (one line, or press Enter for default): \").strip()\n","\n","if not user_scenario:\n","    user_scenario = \"Client needs confidentiality agreement for startup employees, tech sector, California, 50 employees expected.\"\n","    print(f\"Using default scenario: {user_scenario}\")\n","\n","# Select asset type (hardcoded for automation)\n","user_asset_type = \"template\"  # Can be changed to any valid type\n","print(f\"Asset type selected: {user_asset_type}\")\n","\n","# Redact user input\n","redacted_scenario, removed = redact(user_scenario)\n","print(f\"\\nRedaction summary: {len(removed)} field types removed: {removed}\")\n","print(f\"Redacted scenario: {redacted_scenario[:200]}...\" if len(redacted_scenario) > 200 else f\"Redacted scenario: {redacted_scenario}\")\n","\n","# Build user case\n","user_case = {\n","    \"case_id\": \"user_asset\",\n","    \"asset_type\": user_asset_type,\n","    \"facts\": [redacted_scenario],\n","    \"prompt\": f\"\"\"Task: Create {user_asset_type} asset.\n","Asset content limit: 400 words.\n","Return ONLY JSON.\"\"\"\n","}\n","\n","user_dir = DELIVERABLES_DIR / \"user_asset\"\n","\n","try:\n","    print(\"\\nExecuting pipeline...\\n\")\n","\n","    # Stage 1: Generate asset\n","    print(\"  [1/5] Generating asset...\")\n","    user_asset = generate_asset(user_case)\n","    print(f\"    ‚úÖ {user_asset['asset']['title']}\")\n","\n","    # Stage 2: Generate tests (3 tests for user exercise)\n","    print(\"  [2/5] Generating tests...\")\n","    user_tests_prompt = f\"\"\"Task: Generate 3 tests for this asset.\n","Asset type: {user_asset['asset']['asset_type']}\n","Asset title: {user_asset['asset']['title']}\n","Include: edge_case, consistency, clarity tests.\n","Asset content limit: 150 words.\n","Return ONLY JSON.\"\"\"\n","\n","    test_response = call_claude_json_prefill(user_tests_prompt)\n","    user_tests = test_response.get('tests', [])[:3]  # Limit to 3\n","    print(f\"    ‚úÖ Generated {len(user_tests)} tests\")\n","\n","    # Stage 3: Run tests\n","    print(\"  [3/5] Running tests...\")\n","    user_test_results = run_tests(user_asset, user_tests)\n","    passed = sum(1 for t in user_test_results if t.get('pass_fail') == 'pass')\n","    print(f\"    ‚úÖ {passed}/{len(user_test_results)} passed\")\n","\n","    # Stage 4: Revise (if needed)\n","    print(\"  [4/5] Checking if revision needed...\")\n","    user_asset_revised = revise_asset(user_asset, user_test_results)\n","    if user_asset_revised['asset']['version'] == 'v0.2':\n","        user_asset = user_asset_revised\n","        print(\"    ‚úÖ Asset revised to v0.2\")\n","    else:\n","        print(\"    ‚úÖ No revision needed\")\n","\n","    # Stage 5: Build release package\n","    print(\"  [5/5] Building release package...\")\n","    user_package = build_release_package(user_asset, user_tests, user_test_results, user_dir)\n","    print(f\"    ‚úÖ Package created\")\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"USER ASSET COMPLETED\")\n","    print(\"=\"*70)\n","    print(f\"Asset: {user_asset['asset']['title']}\")\n","    print(f\"Version: {user_asset['asset']['version']}\")\n","    print(f\"Tests: {passed}/{len(user_test_results)} passed\")\n","    print(f\"Risks: {len(user_asset.get('risks', []))}\")\n","    print(f\"\\nSaved to: {user_dir}\")\n","    print(\"\\nFiles:\")\n","    for path in user_dir.glob(\"*\"):\n","        print(f\"  - {path.name}\")\n","\n","except Exception as e:\n","    print(f\"\\n‚ùå User exercise failed: {e}\")\n","    print(\"Check error logs in risk_log.json\")"],"metadata":{"id":"JhDf9zHzj3Yr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##10.AUDIT PACKAGE"],"metadata":{"id":"wI0cvSCvj4CQ"}},{"cell_type":"markdown","source":["###10.1.OVERVIEW"],"metadata":{"id":"vzhRfU4Zj5Wl"}},{"cell_type":"markdown","source":["**Create Audit Package and Zip Bundle for Download**\n","\n","This final section completes the governance cycle by packaging all artifacts, logs, and documentation into a comprehensive audit bundle. After executing the pipeline and generating legal assets, this section ensures everything is properly documented, organized, and ready for download, review, and long-term archival. Think of it as closing the case file with a complete table of contents and index.\n","\n","**Updating the Run Manifest**\n","\n","The section begins by reopening the run manifest created at the start and adding final information. It records the end timestamp showing when execution completed, calculates the total duration by comparing start and end times, and appends all the accumulated statistics about API calls, risks logged, cases succeeded and failed, and stages succeeded and failed. This transforms the manifest from an initial plan into a complete execution record.\n","\n","**Creating the Audit README**\n","\n","The system generates a comprehensive README file that serves as the master documentation for the entire audit package. This document contains multiple sections explaining what the package contains and how to use it. The run information section summarizes when execution occurred, which model was used, and what the notebook's purpose was. The statistics section presents the quantitative performance data in readable format. The directory structure section provides a visual map of all folders and files.\n","\n","**Documentation of Governance Artifacts**\n","\n","The README explains each governance artifact in detail. It describes what the run manifest contains and why it matters. It explains that the prompts log uses line-delimited JSON format with redacted content and cryptographic hashes. It clarifies that the risk log aggregates all identified risks for centralized review. It details what each case's deliverables directory contains, including versioned assets, test suites, test results, release manifests, and human review checklists.\n","\n","**Critical Reminders Section**\n","\n","The README includes prominent warnings that must accompany any AI-generated legal work. It emphasizes that all outputs are drafts requiring human lawyer review, that verification status is set to \"Not verified\" on everything, that human review is mandatory before any use, that redaction is imperfect so logs should be reviewed before sharing, and that no citations or authorities were invented. These warnings protect against the most common risks when using AI tools in legal practice.\n","\n","**Next Steps Guidance**\n","\n","The README provides concrete instructions for what a human attorney should do with this package. Review each human review checklist systematically. Verify all items marked in questions to verify sections. Check the risk log for high severity flags requiring immediate attention. Consult a supervising attorney before deploying any assets. Update the release readiness status after completing human review. Archive the audit bundle for record-keeping and potential future reference.\n","\n","**Contact Information**\n","\n","The README includes contact information for the notebook's author, establishing accountability and providing a resource for questions or concerns. This personal attribution reinforces that the system is a tool created by identifiable professionals, not an anonymous black box.\n","\n","**Creating the Zip Bundle**\n","\n","After generating the README, the system creates a compressed zip archive containing the entire run directory with all its contents. The zip file name includes the timestamp, making it unique and clearly associated with this specific execution. The compression makes the bundle easier to download, transfer, and archive while keeping all related files together as a single unit.\n","\n","**File Tree Display**\n","\n","The section prints a complete hierarchical listing of every file and directory in the audit package. This visual representation helps you understand the organization at a glance. You see the top-level governance files like run manifest, prompts log, risk log, and pip freeze. You see the deliverables folder containing subdirectories for each case. Within each case directory, you see the versioned asset files, test files, manifest, and checklist.\n","\n","**Download Instructions**\n","\n","The system provides explicit step-by-step instructions for downloading the zip bundle from Google Colab. Click the folder icon in the left sidebar to open the file browser. Navigate to the specified zip file path. Right-click and select download. These concrete instructions ensure even users unfamiliar with Colab can successfully retrieve their audit package.\n","\n","**Bundle Size Display**\n","\n","The section displays the zip file size in kilobytes, giving you a sense of the package's scope. A typical run might produce fifty to one hundred kilobytes depending on how much content was generated and how many API calls were made. This size information helps you anticipate download time and storage requirements.\n","\n","**Governance Checklist**\n","\n","The final output presents a checklist of governance tasks that a human attorney should complete. Review all human review checklists from each case. Verify questions requiring verification. Check the risk log for high severity items. Consult a supervising attorney before any deployment. Update release readiness status after review. Archive the audit bundle for compliance and record-keeping. This checklist transforms abstract governance requirements into concrete actionable steps.\n","\n","**Completion Confirmation**\n","\n","The section ends with a clear confirmation that the Chapter Four pipeline is complete. This explicit closure provides psychological satisfaction and confirms that all intended work has been performed. You are not left wondering whether more steps remain or whether something failed silently.\n","\n","**Why Comprehensive Packaging Matters**\n","\n","For legal practice, this thorough packaging addresses multiple professional responsibilities simultaneously. It creates the audit trail that ethical rules require when using technology in practice. It generates documentation that could respond to client questions, opposing counsel inquiries, or regulatory examinations. It provides the materials a supervising attorney needs for effective review. It ensures that six months or six years later, you can reconstruct exactly what was done, why it was done, and what safeguards were in place.\n","\n","**Long-Term Archival Value**\n","\n","The audit package is designed for long-term preservation. The README ensures future readers can understand the contents even if they were not involved in the original execution. The manifest provides context about when and why this work was performed. The environment snapshot enables technical reproduction if necessary. The versioned assets show the evolution from initial draft to revised version. Together, these elements create a self-contained historical record that maintains its value over time.\n","\n","**Professional Standard Demonstration**\n","\n","This comprehensive packaging demonstrates what responsible AI use in legal practice should look like. It is not enough to generate useful outputs. You must also document what you did, track what risks were identified, enable verification of results, and create an audit trail that demonstrates due diligence. This final section transforms the notebook from a useful tool into a governance-compliant professional workflow."],"metadata":{"id":"UtT7WhulkdQp"}},{"cell_type":"markdown","source":["###10.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"vHS-9n2yj69I"}},{"cell_type":"code","source":["# Create AUDIT_README.txt and zip bundle for download\n","\n","print(\"Creating final audit package...\\n\")\n","\n","# Update run manifest with end time\n","with open(manifest_path, 'r') as f:\n","    final_manifest = json.load(f)\n","\n","final_manifest['end_time'] = datetime.now().isoformat()\n","final_manifest['stats'] = stats\n","\n","with open(manifest_path, 'w') as f:\n","    json.dump(final_manifest, f, indent=2)\n","\n","# Create AUDIT_README.txt\n","audit_readme = f\"\"\"AI LAW CHAPTER 4 - LEVEL 4 INNOVATORS\n","AUDIT PACKAGE README\n","{'='*70}\n","\n","RUN INFORMATION:\n","  Run ID: {timestamp}\n","  Model: {MODEL}\n","  Chapter: 4 - Innovators (Reusable Legal Assets)\n","  Start: {final_manifest['start_time']}\n","  End: {final_manifest['end_time']}\n","\n","STATISTICS:\n","  Cases completed: {stats['cases_success']}/{stats['cases_success'] + stats['cases_fail']}\n","  Pipeline stages: {stats['stages_success']} success, {stats['stages_fail']} failed\n","  Total API calls: {stats['total_api_calls']}\n","  Total risks logged: {stats['total_risks_logged']}\n","\n","DIRECTORY STRUCTURE:\n","  run_{timestamp}/\n","  ‚îú‚îÄ‚îÄ run_manifest.json         (Run metadata + stats)\n","  ‚îú‚îÄ‚îÄ prompts_log.jsonl         (Redacted prompts/responses + hashes)\n","  ‚îú‚îÄ‚îÄ risk_log.json             (Aggregated risk findings)\n","  ‚îú‚îÄ‚îÄ pip_freeze.txt            (Python dependencies)\n","  ‚îú‚îÄ‚îÄ AUDIT_README.txt          (This file)\n","  ‚îî‚îÄ‚îÄ deliverables/\n","      ‚îú‚îÄ‚îÄ criminal_defense_intake/\n","      ‚îÇ   ‚îú‚îÄ‚îÄ asset_v0.1.json (or v0.2 if revised)\n","      ‚îÇ   ‚îú‚îÄ‚îÄ tests.json\n","      ‚îÇ   ‚îú‚îÄ‚îÄ test_results.json\n","      ‚îÇ   ‚îú‚îÄ‚îÄ release_manifest.json\n","      ‚îÇ   ‚îî‚îÄ‚îÄ human_review_checklist.txt\n","      ‚îú‚îÄ‚îÄ regulatory_comment_letter/\n","      ‚îú‚îÄ‚îÄ cross_border_dispute/\n","      ‚îú‚îÄ‚îÄ course_ai_policy/\n","      ‚îî‚îÄ‚îÄ user_asset/ (if user exercise completed)\n","\n","GOVERNANCE ARTIFACTS:\n","\n","1. run_manifest.json\n","   - Run ID, model, timestamps\n","   - Final statistics\n","   - Purpose and chapter\n","\n","2. prompts_log.jsonl\n","   - Line-delimited JSON log\n","   - Each entry: timestamp, prompt hash, redacted prompt/response\n","   - NO unredacted PII\n","\n","3. risk_log.json\n","   - Aggregated risks from all API calls\n","   - Type, severity, notes\n","   - Used for post-run analysis\n","\n","4. deliverables/<case_id>/\n","   - Versioned assets (v0.1, v0.2)\n","   - Test suite + results\n","   - Release manifest (readiness assessment)\n","   - Human review checklist (verification items)\n","\n","CRITICAL REMINDERS:\n","\n","‚ö†Ô∏è  ALL OUTPUTS ARE DRAFTS - NOT LEGAL ADVICE\n","‚ö†Ô∏è  verification_status = \"Not verified\" ON ALL ASSETS\n","‚ö†Ô∏è  HUMAN LAWYER REVIEW MANDATORY BEFORE USE\n","‚ö†Ô∏è  REDACTION IS IMPERFECT - Review logs before sharing\n","‚ö†Ô∏è  NO INVENTED CITATIONS - All authority marked \"Not verified\"\n","\n","NEXT STEPS:\n","\n","1. Review human_review_checklist.txt for each asset\n","2. Verify all items in questions_to_verify\n","3. Check risk_log.json for high-severity flags\n","4. Consult supervising attorney before deployment\n","5. Update release_readiness.status after human review\n","\n","QUESTIONS OR CONCERNS:\n","Contact: Alejandro Reynoso\n","Position: Chief Scientist DEFI CAPITAL RESEARCH and External Lecturer,\n","          Judge Business School Cambridge\n","\n","{'='*70}\n","Generated: {datetime.now().isoformat()}\n","\"\"\"\n","\n","audit_readme_path = RUN_DIR / \"AUDIT_README.txt\"\n","with open(audit_readme_path, 'w') as f:\n","    f.write(audit_readme)\n","\n","print(f\"‚úÖ AUDIT_README.txt created: {audit_readme_path}\")\n","\n","# Create zip bundle\n","zip_path = Path(f\"/content/ai_law_ch4_run_{timestamp}.zip\")\n","shutil.make_archive(str(zip_path.with_suffix('')), 'zip', RUN_DIR)\n","\n","print(f\"‚úÖ Zip bundle created: {zip_path}\")\n","\n","# Print file list\n","print(\"\\n\" + \"=\"*70)\n","print(\"AUDIT PACKAGE FILE LIST\")\n","print(\"=\"*70 + \"\\n\")\n","\n","for root, dirs, files in os.walk(RUN_DIR):\n","    level = root.replace(str(RUN_DIR), '').count(os.sep)\n","    indent = '  ' * level\n","    print(f\"{indent}{os.path.basename(root)}/\")\n","    subindent = '  ' * (level + 1)\n","    for file in files:\n","        print(f\"{subindent}{file}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"DOWNLOAD BUNDLE\")\n","print(\"=\"*70)\n","print(f\"\\nZip file ready for download: {zip_path}\")\n","print(f\"Size: {zip_path.stat().st_size / 1024:.1f} KB\")\n","print(\"\\nTo download:\")\n","print(\"  1. Click the folder icon üìÅ in the left sidebar\")\n","print(f\"  2. Navigate to: {zip_path}\")\n","print(\"  3. Right-click ‚Üí Download\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"GOVERNANCE CHECKLIST\")\n","print(\"=\"*70)\n","print(\"\\n[ ] Review all human_review_checklist.txt files\")\n","print(\"[ ] Verify questions_to_verify in each asset\")\n","print(\"[ ] Check risk_log.json for high-severity items\")\n","print(\"[ ] Consult supervising attorney\")\n","print(\"[ ] Update release readiness status\")\n","print(\"[ ] Archive audit bundle for record-keeping\")\n","print(\"\\n‚úÖ Chapter 4 pipeline complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wPDfj-45j8c5","executionInfo":{"status":"ok","timestamp":1767878110756,"user_tz":360,"elapsed":21,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"2fccd861-988a-4cbb-d483-03f31875fd10"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating final audit package...\n","\n","‚úÖ AUDIT_README.txt created: /content/ai_law_ch4_runs/run_20260108_131043/AUDIT_README.txt\n","‚úÖ Zip bundle created: /content/ai_law_ch4_run_20260108_131043.zip\n","\n","======================================================================\n","AUDIT PACKAGE FILE LIST\n","======================================================================\n","\n","run_20260108_131043/\n","  AUDIT_README.txt\n","  deliverables/\n","\n","======================================================================\n","DOWNLOAD BUNDLE\n","======================================================================\n","\n","Zip file ready for download: /content/ai_law_ch4_run_20260108_131043.zip\n","Size: 1.5 KB\n","\n","To download:\n","  1. Click the folder icon üìÅ in the left sidebar\n","  2. Navigate to: /content/ai_law_ch4_run_20260108_131043.zip\n","  3. Right-click ‚Üí Download\n","\n","======================================================================\n","GOVERNANCE CHECKLIST\n","======================================================================\n","\n","[ ] Review all human_review_checklist.txt files\n","[ ] Verify questions_to_verify in each asset\n","[ ] Check risk_log.json for high-severity items\n","[ ] Consult supervising attorney\n","[ ] Update release readiness status\n","[ ] Archive audit bundle for record-keeping\n","\n","‚úÖ Chapter 4 pipeline complete!\n"]}]},{"cell_type":"markdown","source":["##11.CONCLUSIONS"],"metadata":{"id":"GacXZ2Fzj9V8"}},{"cell_type":"markdown","source":["**Complete Pipeline Overview: From Initialization to Audit Bundle**\n","\n","This notebook implements a comprehensive five-stage pipeline for creating, testing, and releasing reusable legal assets with full governance tracking. Understanding how all ten sections work together reveals a sophisticated system that balances automation with accountability, efficiency with safety, and innovation with professional responsibility.\n","\n","**Foundation Layer: Setup and Infrastructure**\n","\n","The pipeline begins with foundational setup across the first four sections. Section one provides the conceptual framework, explaining that reusable legal assets have increased blast radius requiring scaled governance. Section two installs dependencies and creates a timestamped run directory where all artifacts will be stored. Section three establishes the API connection to Claude, loading credentials securely from Colab Secrets and initializing the client with the specified Haiku model. Section four creates the governance infrastructure: run manifest for metadata tracking, prompts log for redacted API interactions, risk log for aggregated findings, environment snapshot for reproducibility, and statistics tracker for performance monitoring. These four sections build the foundation that everything else depends upon.\n","\n","**Privacy Protection Layer**\n","\n","Section five implements redaction utilities that protect confidential information throughout the pipeline. The redaction function scans for emails, phone numbers, Social Security numbers, and street addresses, replacing them with labeled placeholders. The section demonstrates this protection with fake data, showing before and after states while emphasizing that redaction is imperfect and real sensitive data should never be entered. This privacy layer gets invoked repeatedly throughout later stages, ensuring that facts sent to the API, content written to logs, and responses stored in files all undergo redaction before any external transmission or permanent storage.\n","\n","**Reliability Layer: Prefill-Enforced JSON**\n","\n","Section six solves the critical technical challenge of obtaining consistent structured outputs from a conversational AI model. The prefill technique forces Claude to start its response with an opening brace, committing it to JSON completion mode rather than explanatory conversation mode. Four fallback extraction strategies provide additional safety nets if prefill alone proves insufficient. Schema validation ensures all required keys are present. Automatic risk flagging scans for hallucination indicators, missing disclaimers, and overconfident verification claims. Retry logic with progressive strictness attempts up to three times with increasing constraints. Error fallback schema ensures valid outputs even when parsing completely fails. A smoke test validates the entire mechanism before main execution begins. This elaborate defensive programming creates the reliability foundation necessary for professional legal practice.\n","\n","**Business Logic Layer: Cases and Pipeline Functions**\n","\n","Section seven defines what actual legal work the notebook performs. Four mini-case builders create realistic scenarios across criminal defense, regulatory practice, international transactions, and legal education. Each builder returns concrete facts with names and numbers, a minimal directive prompt, and metadata about case type. The five pipeline stage functions then define the transformation process. Generate asset produces the initial version zero point one draft with required disclaimers and structure. Generate tests creates adversarial and edge case evaluations designed to stress-test the asset. Run tests executes those evaluations through simulated assessment, returning pass-fail results and remediation notes. Revise asset performs a single improvement iteration if any tests failed, producing version zero point two. Build release package creates the complete artifact set including versioned JSON files, test documentation, release manifest, and human review checklist. These functions implement the core asset development methodology.\n","\n","**Execution Layer: Running the Pipeline**\n","\n","Section eight brings everything together by executing all four cases through the complete five-stage pipeline. The execution loop processes each case sequentially, wrapping every stage in error handling that isolates failures and allows other cases to continue. Visible progress indicators show exactly what stage is executing and whether it succeeded or failed. Statistics accumulate across all cases, tracking API calls, risks logged, cases completed, and stages completed. Test results get analyzed to calculate pass rates and determine whether revision is necessary. Risk severities get assessed to identify which cases need most urgent human attention. After processing all cases, a comprehensive summary table displays case-by-case outcomes, and aggregate statistics provide overall performance metrics. The deliverables directory path is explicitly shown so outputs can be located for review.\n","\n","**Interactive Learning Layer**\n","\n","Section nine transforms the demonstration into hands-on experience by letting you create your own custom asset. You enter a scenario describing your legal situation, select an asset type, and watch as redaction occurs on your input. Your custom case then flows through the same five-stage pipeline as the predefined examples, though with abbreviated testing for efficiency. You see your asset generated, tested with three evaluations, potentially revised, and packaged with full governance artifacts. This interactive exercise reinforces conceptual understanding through direct experience while demonstrating that the infrastructure handles novel scenarios just as reliably as predefined examples.\n","\n","**Documentation and Archival Layer**\n","\n","Section ten completes the governance cycle by packaging everything for audit, review, and long-term preservation. The run manifest gets updated with end timestamps and final statistics. A comprehensive README document explains the entire package structure, describes each artifact type, provides critical warnings about verification requirements, and offers concrete next-steps guidance for human attorneys. The entire run directory gets compressed into a timestamped zip bundle. A visual file tree shows the complete organization. Download instructions provide step-by-step guidance for retrieving the bundle from Colab. A governance checklist translates abstract requirements into actionable review tasks. The final confirmation signals completion and readiness for human review.\n","\n","**The Complete Flow**\n","\n","Following a single case through the entire pipeline reveals the orchestration. Infrastructure initializes. A case builder provides facts and prompt. Redaction protects privacy. The prefill-enforced API call generates a structured asset draft. Another API call creates adversarial tests. Multiple API calls execute those tests through simulated evaluation. If tests fail, another API call revises the asset. File operations save all artifacts with version numbers and manifests. Logs capture every interaction in redacted form. Risks aggregate to a central repository. Statistics track performance. Finally, everything packages into a downloadable audit bundle with comprehensive documentation. This end-to-end flow demonstrates how technical reliability, privacy protection, quality assurance, and governance documentation integrate seamlessly into a single coherent workflow suitable for professional legal practice."],"metadata":{"id":"kZuwgyonr96x"}}]}